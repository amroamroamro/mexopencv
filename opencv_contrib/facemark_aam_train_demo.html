<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>Facemark AAM training demo</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2018-02-21">
      <meta name="DC.source" content="facemark_aam_train_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">Facemark AAM training demo</h1>
         <!--introduction-->
         <p>The user should provides the list of training images accompanied by their corresponding landmarks location in separate files.</p>
         <p>See below for a description of file formats.</p>
         <p>Examples of datasets are available at <a href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/">https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/</a>.
         </p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/samples/facemark_demo_aam.cpp">https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/samples/facemark_demo_aam.cpp</a></li>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/tutorials/facemark_aam/facemark_aam.markdown">https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/tutorials/facemark_aam/facemark_aam.markdown</a></li>
            </ul>
         </div>
         <!--/introduction-->
         <h2 id="toc">Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Preparation</a></li>
               <li><a href="#3">Options</a></li>
               <li><a href="#4">Init</a></li>
               <li><a href="#6">Data</a></li>
               <li><a href="#7">Train</a></li>
               <li><a href="#8">Prepare for Test</a></li>
               <li><a href="#14">Test</a></li>
               <li><a href="#16">Helper functions</a></li>
            </ul>
         </div>
         <h2 id="2">Preparation</h2>
         <p>Before you continue with this tutorial, you should download a training dataset of facial landmarks detection.</p>
         <p>We suggest you to download the LFPW dataset which can be retrieved at <a href="https://ibug.doc.ic.ac.uk/download/annotations/lfpw.zip">https://ibug.doc.ic.ac.uk/download/annotations/lfpw.zip</a>.
         </p>
         <p>First thing to do is to make two text files containing the list of image files and annotation files respectively. Make sure
            that the order of images and annotations in both files are matched. Furthermore, it is advised to use absolute paths instead
            of relative paths.
         </p>
         <p>Example to make the file list in Linux machine:</p><pre>ls /data/lfpw/trainset/*.png &gt; images_train.txt
ls /data/lfpw/trainset/*.pts &gt; annotations_train.txt</pre><p>Optionally, you can also create similar files list for the testset.</p>
         <p>Example of content in the <tt>images_train.txt</tt> file:
         </p><pre>/data/lfpw/trainset/image_0001.png
/data/lfpw/trainset/image_0002.png
/data/lfpw/trainset/image_0003.png
...</pre><p>Example of content in the <tt>annotations_train.txt</tt> file:
         </p><pre>/data/lfpw/trainset/image_0001.pts
/data/lfpw/trainset/image_0002.pts
/data/lfpw/trainset/image_0003.pts
...</pre><p>where a <tt>.pts</tt> file contains the position of each face landmark. Make sure that the annotation format is supported by the API, where the
            contents should look like the following snippet:
         </p><pre>version: 1
n_points:  68
{
212.716603 499.771793
230.232816 566.290071
...
}</pre><p>Once trained, we show how to use the model to detect face landmarks in a test image.</p>
         <p>In this tutorial, the pre-trained model will not be provided due to its large file size (~500MB). By following this tutorial,
            you will be able to train and obtain your own trained model within few minutes.
         </p>
         <h2 id="3">Options</h2><pre class="codeinput"><span class="comment">% [INPUT] path of a text file contains the list of paths to all training images</span>
imgList = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'facemark'</span>,<span class="string">'lfpw'</span>,<span class="string">'images.lst'</span>);
assert(exist(imgList, <span class="string">'file'</span>) == 2, <span class="string">'missing images list file'</span>);

<span class="comment">% [INPUT] path of a text file contains the list of paths to all annotations files</span>
ptsList = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'facemark'</span>,<span class="string">'lfpw'</span>,<span class="string">'annotations.lst'</span>);
assert(exist(ptsList, <span class="string">'file'</span>) == 2, <span class="string">'missing annotations list file'</span>);

<span class="comment">% [OUTPUT] path for saving the trained model</span>
modelFile = fullfile(tempdir(), <span class="string">'model_aam.yaml'</span>);

<span class="comment">% [INPUT] path to the cascade xml file for the face detector</span>
xmlFace = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'haarcascade_frontalface_alt.xml'</span>);
download_classifier_xml(xmlFace);

<span class="comment">% [INPUT] path to the cascade xml file for the eyes detector</span>
xmlEyes = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'haarcascade_eye_tree_eyeglasses.xml'</span>);
download_classifier_xml(xmlEyes);

<span class="comment">% path to test image</span>
testImg = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'lena.jpg'</span>);</pre><h2 id="4">Init</h2>
         <p>create the facemark instance</p><pre class="codeinput">scales = [2.0, 4.0];
obj = cv.Facemark(<span class="string">'AAM'</span>, <span class="string">'Scales'</span>,scales, <span class="keyword">...</span>
    <span class="string">'ModelFilename'</span>,modelFile, <span class="string">'SaveModel'</span>,true, <span class="string">'Verbose'</span>,true);</pre><p>In this case, we modified the default list of the scaling factor. By default, the scaling factor used is 1.0 (no scaling).
            Here we add two more scaling factor which will make the instance trains two more model at scale 2 and 4 (2 times smaller and
            4 times smaller, with faster fitting time). However, you should make sure that this scaling factor is not too big since it
            will make the image scaled into a very small one. Thus it will lose all of its important information for the landmark detection
            purpose.
         </p>
         <h2 id="6">Data</h2>
         <p>load the dataset, and add training samples one-by-one</p><pre class="codeinput">disp(<span class="string">'Loading data...'</span>)
[imgFiles, ptsFiles] = cv.Facemark.loadDatasetList(imgList, ptsList);
<span class="keyword">for</span> i=1:numel(imgFiles)
    <span class="comment">% load image and its corresponding annotation data, then add pair</span>
    img = cv.imread(imgFiles{i});
    pts = cv.Facemark.loadFacePoints(ptsFiles{i});
    obj.addTrainingSample(img, pts);
<span class="keyword">end</span></pre><pre class="codeoutput">Loading data...
</pre><h2 id="7">Train</h2>
         <p>train the algorithm, model will be saved to specified file</p><pre class="codeinput">disp(<span class="string">'Training...'</span>)
tic
obj.training();
toc</pre><pre class="codeoutput">Training...
Elapsed time is 6.612023 seconds.
</pre><h2 id="8">Prepare for Test</h2>
         <p>Since the AAM algorithm needs initialization parameters (rotation, translation, and scaling), we need to declare the required
            variable to store these information which will be obtained using a custom function. The implementation of <tt>getInitialFitting</tt> function in this example is not optimal, you can always create your own function.
         </p>
         <p>The initialization is obtained by comparing the base shape of the trained model with the current face image. In this case,
            the rotation is obtained by comparing the angle of line formed by two eyes in the input face image with the same line in the
            base shape. Meanwhile, the scaling is obtained by comparing the length of line between eyes in the input image compared to
            the base shape.
         </p>
         <p>The fitting process starts by detecting faces in given image.</p>
         <p>If at least one face is found, then the next step is computing the initialization parameters. In this case, since <tt>getInitialFitting</tt> function is not optimal, it may not find pair of eyes from a given face. Therefore, we will filter out faces without initialization
            parameters and in this case, each element in the <tt>confs</tt> vector represent the initialization parameters for each filtered face.
         </p>
         <p>create cascade detector objects (for face and eyes)</p><pre class="codeinput">ccFace = cv.CascadeClassifier(xmlFace);
ccEyes = cv.CascadeClassifier(xmlEyes);</pre><p>detect faces</p><pre class="codeinput">img = cv.imread(testImg);
faces = myFaceDetector(img, ccFace);
assert(~isempty(faces), <span class="string">'no faces found'</span>);
fprintf(<span class="string">'%d faces\n'</span>, numel(faces));</pre><pre class="codeoutput">1 faces
</pre><p>get base shape from trained model</p><pre class="codeinput">s0 = obj.getData();
s0 = cat(1, s0{:});</pre><p>compute initialization params for each detected face</p><pre class="codeinput">S = struct(<span class="string">'R'</span>,eye(2), <span class="string">'t'</span>,[0 0], <span class="string">'scale'</span>,1);
confs = S([]);
faces_eyes = {};
<span class="keyword">for</span> i=1:numel(faces)
    [conf, found] = getInitialFitting(img, faces{i}, s0, ccEyes);
    <span class="keyword">if</span> found
        confs(end+1) = conf;
        faces_eyes{end+1} = faces{i};
    <span class="keyword">end</span>
<span class="keyword">end</span>
assert(~isempty(confs), <span class="string">'failed to compute initialization params'</span>);
fprintf(<span class="string">'%d faces with eyes\n'</span>, numel(confs));</pre><pre class="codeoutput">1 faces with eyes
</pre><p>For the fitting parameters stored in the <tt>confs</tt> vector, <tt>scaleIdx</tt> field represents the ID of scaling factor that will be used in the fitting process. In this example the fitting will use
            the biggest scaling factor (4) which is expected to have the fastest computation time compared to the other scales. If the
            ID is bigger than the available trained scales in the model, the model with the biggest scale ID is used.
         </p><pre class="codeinput">confs.scaleIdx = numel(scales) - 1;</pre><h2 id="14">Test</h2>
         <p>The fitting process is quite simple, you just need to pass the image, array of rectangles representing the ROIs of all faces
            in the given image, and the configuration params. It returns the landmark points.
         </p><pre class="codeinput">tic
landmarks = obj.fit(img, faces_eyes, <span class="string">'Configs'</span>,confs);
toc</pre><pre class="codeoutput">Elapsed time is 0.481008 seconds.
</pre><p>After the fitting process is finished, we can visualize the result</p><pre class="codeinput"><span class="keyword">for</span> i=1:numel(landmarks)
    img = cv.Facemark.drawFacemarks(img, landmarks{i}, <span class="string">'Color'</span>,[0 255 0]);
<span class="keyword">end</span>
imshow(img)</pre><img src="facemark_aam_train_demo_01.png"><h2 id="16">Helper functions</h2><pre class="codeinput"><span class="keyword">function</span> download_classifier_xml(fname)
    <span class="keyword">if</span> exist(fname, <span class="string">'file'</span>) ~= 2
        <span class="comment">% attempt to download trained Haar/LBP/HOG classifier from Github</span>
        url = <span class="string">'https://cdn.rawgit.com/opencv/opencv/3.4.0/data/'</span>;
        [~, f, ext] = fileparts(fname);
        <span class="keyword">if</span> strncmpi(f, <span class="string">'haarcascade_'</span>, length(<span class="string">'haarcascade_'</span>))
            url = [url, <span class="string">'haarcascades/'</span>];
        <span class="keyword">elseif</span> strncmpi(f, <span class="string">'lbpcascade_'</span>, length(<span class="string">'lbpcascade_'</span>))
            url = [url, <span class="string">'lbpcascades/'</span>];
        <span class="keyword">elseif</span> strncmpi(f, <span class="string">'hogcascade_'</span>, length(<span class="string">'hogcascade_'</span>))
            url = [url, <span class="string">'hogcascades/'</span>];
        <span class="keyword">else</span>
            error(<span class="string">'File not found'</span>);
        <span class="keyword">end</span>
        urlwrite([url f ext], fname);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> faces = myFaceDetector(img, ccFace)
    <span class="comment">%MYFACEDETECTOR  Detect faces</span>
    <span class="comment">%</span>
    <span class="comment">%    faces = myFaceDetector(img, ccFace)</span>
    <span class="comment">%</span>
    <span class="comment">% ## Input</span>
    <span class="comment">% * __img__ input image</span>
    <span class="comment">% * __ccFace__ cascade object for face detection</span>
    <span class="comment">%</span>
    <span class="comment">% ## Output</span>
    <span class="comment">% * __faces__ detected faces, `{[x,y,w,h], ...}`</span>
    <span class="comment">%</span>
    <span class="comment">% See also: cv.Facemark.getFacesHAAR</span>
    <span class="comment">%</span>

    <span class="keyword">if</span> size(img,3) &gt; 1
        gray = cv.cvtColor(img, <span class="string">'RGB2GRAY'</span>);
    <span class="keyword">else</span>
        gray = img;
    <span class="keyword">end</span>
    gray = cv.equalizeHist(gray);
    faces = ccFace.detect(gray, <span class="string">'ScaleFactor'</span>,1.4, <span class="string">'MinNeighbors'</span>,2, <span class="keyword">...</span>
        <span class="string">'ScaleImage'</span>,true, <span class="string">'MinSize'</span>,[30 30]);
<span class="keyword">end</span>

<span class="keyword">function</span> [conf, found] = getInitialFitting(img, face, s0, ccEyes)
    <span class="comment">%GETINITIALFITTING  Calculate AAM intial fit params</span>
    <span class="comment">%</span>
    <span class="comment">%     [conf, found] = getInitialFitting(img, face, s0, ccEyes)</span>
    <span class="comment">%</span>
    <span class="comment">% ## Input</span>
    <span class="comment">% * __img__ input image</span>
    <span class="comment">% * __face__ detected face `[x,y,w,h]`</span>
    <span class="comment">% * __s0__ base shape of the trained model</span>
    <span class="comment">% * __ccEyes__ cascade object for eyes detection</span>
    <span class="comment">%</span>
    <span class="comment">% ## Output</span>
    <span class="comment">% * __conf__ struct with rotation, translation, and scale</span>
    <span class="comment">% * __found__ success flag</span>
    <span class="comment">%</span>

    found = false;
    conf = struct(<span class="string">'R'</span>,eye(2), <span class="string">'t'</span>,[0 0], <span class="string">'scale'</span>,1.0);

    <span class="comment">% detect eyes in face</span>
    <span class="keyword">if</span> cv.Rect.area(face) == 0, <span class="keyword">return</span>; <span class="keyword">end</span>
    faceROI = cv.Rect.crop(img, face);
    eyes = ccEyes.detect(faceROI, <span class="string">'ScaleFactor'</span>,1.1, <span class="string">'MinNeighbors'</span>,2, <span class="keyword">...</span>
        <span class="string">'ScaleImage'</span>,true, <span class="string">'MinSize'</span>,[20 20]);
    <span class="keyword">if</span> numel(eyes) ~= 2, <span class="keyword">return</span>; <span class="keyword">end</span>

    <span class="comment">% make sure that first is left eye, second is right eye</span>
    <span class="keyword">if</span> eyes{2}(1) &lt; eyes{1}(1)
        eyes = eyes([2 1]);
    <span class="keyword">end</span>

    <span class="comment">% eyes centers in detected face</span>
    c1 = face(1:2) + eyes{1}(1:2) + eyes{1}(3:4)/2;  <span class="comment">% left eye</span>
    c2 = face(1:2) + eyes{2}(1:2) + eyes{2}(3:4)/2;  <span class="comment">% right eye</span>
    assert(c1(1) &lt; c2(1), <span class="string">'eyes not ordered correctly (left then right)'</span>);

    <span class="comment">% eyes centers in base shape (shifted to middle of image)</span>
    base = bsxfun(@plus, s0, [size(img,2) size(img,1)]/2);
    c1Base = (base(37,:) + base(40,:)) / 2;  <span class="comment">% left eye</span>
    c2Base = (base(43,:) + base(46,:)) / 2;  <span class="comment">% right eye</span>

    <span class="comment">% scale between the two line length in detected and base shape</span>
    scale = norm(c2 - c1) / norm(c2Base - c1Base);

    <span class="comment">% eyes centers in scaled base shape (not shifted)</span>
    base = s0 * scale;
    c1Base = (base(37,:) + base(40,:)) / 2;
    c2Base = (base(43,:) + base(46,:)) / 2;

    <span class="comment">% angle of horizontal line connecting eyes centers in scaled base shape</span>
    aBase = atan2(c2Base(2) - c1Base(2), c2Base(1) - c1Base(1));

    <span class="comment">% angle of horizontal line connecting eyes centers in detect face</span>
    a = atan2(c2(2) - c1(2), c2(1) - c1(1));

    <span class="comment">% rotation matrix from the two angles</span>
    R = cv.getRotationMatrix2D([0 0], rad2deg(aBase-a), 1.0);
    R = R(1:2,1:2);

    <span class="comment">% eyes centers in transformed base shape (scaled then rotated)</span>
    base = (R * scale * s0')';
    c1Base = (base(37,:) + base(40,:)) / 2;
    c2Base = (base(43,:) + base(46,:)) / 2;

    <span class="comment">% translation between detected and transformed base shape</span>
    t = c1 - c1Base;

    <span class="comment">% fill output</span>
    found = true;
    conf.R = R;
    conf.t = t;
    conf.scale = scale;
<span class="keyword">end</span></pre><div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Facemark AAM training demo
%
% The user should provides the list of training images accompanied by their
% corresponding landmarks location in separate files.
%
% See below for a description of file formats.
%
% Examples of datasets are available at
% <https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/>.
%
% Sources:
%
% * <https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/samples/facemark_demo_aam.cpp>
% * <https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/tutorials/facemark_aam/facemark_aam.markdown>
%

%% Preparation
%
% Before you continue with this tutorial, you should download a training
% dataset of facial landmarks detection.
%
% We suggest you to download the LFPW dataset which can be retrieved at
% <https://ibug.doc.ic.ac.uk/download/annotations/lfpw.zip>.
%
% First thing to do is to make two text files containing the list of image
% files and annotation files respectively. Make sure that the order of images
% and annotations in both files are matched. Furthermore, it is advised to use
% absolute paths instead of relative paths.
%
% Example to make the file list in Linux machine:
%
%  ls /data/lfpw/trainset/*.png > images_train.txt
%  ls /data/lfpw/trainset/*.pts > annotations_train.txt
%
% Optionally, you can also create similar files list for the testset.
%
% Example of content in the |images_train.txt| file:
%
%  /data/lfpw/trainset/image_0001.png
%  /data/lfpw/trainset/image_0002.png
%  /data/lfpw/trainset/image_0003.png
%  ...
%
% Example of content in the |annotations_train.txt| file:
%
%  /data/lfpw/trainset/image_0001.pts
%  /data/lfpw/trainset/image_0002.pts
%  /data/lfpw/trainset/image_0003.pts
%  ...
%
% where a |.pts| file contains the position of each face landmark.
% Make sure that the annotation format is supported by the API, where the
% contents should look like the following snippet:
%
%  version: 1
%  n_points:  68
%  {
%  212.716603 499.771793
%  230.232816 566.290071
%  ...
%  }
%
% Once trained, we show how to use the model to detect face landmarks in a
% test image.
%
% In this tutorial, the pre-trained model will not be provided due to its
% large file size (~500MB). By following this tutorial, you will be able to
% train and obtain your own trained model within few minutes.
%

%% Options

% [INPUT] path of a text file contains the list of paths to all training images
imgList = fullfile(mexopencv.root(),'test','facemark','lfpw','images.lst');
assert(exist(imgList, 'file') == 2, 'missing images list file');

% [INPUT] path of a text file contains the list of paths to all annotations files
ptsList = fullfile(mexopencv.root(),'test','facemark','lfpw','annotations.lst');
assert(exist(ptsList, 'file') == 2, 'missing annotations list file');

% [OUTPUT] path for saving the trained model
modelFile = fullfile(tempdir(), 'model_aam.yaml');

% [INPUT] path to the cascade xml file for the face detector
xmlFace = fullfile(mexopencv.root(),'test','haarcascade_frontalface_alt.xml');
download_classifier_xml(xmlFace);

% [INPUT] path to the cascade xml file for the eyes detector
xmlEyes = fullfile(mexopencv.root(),'test','haarcascade_eye_tree_eyeglasses.xml');
download_classifier_xml(xmlEyes);

% path to test image
testImg = fullfile(mexopencv.root(),'test','lena.jpg');

%% Init
% create the facemark instance
scales = [2.0, 4.0];
obj = cv.Facemark('AAM', 'Scales',scales, ...
    'ModelFilename',modelFile, 'SaveModel',true, 'Verbose',true);

%%
% In this case, we modified the default list of the scaling factor.
% By default, the scaling factor used is 1.0 (no scaling). Here we add two
% more scaling factor which will make the instance trains two more model at
% scale 2 and 4 (2 times smaller and 4 times smaller, with faster fitting
% time). However, you should make sure that this scaling factor is not too
% big since it will make the image scaled into a very small one. Thus it will
% lose all of its important information for the landmark detection purpose.

%% Data
% load the dataset, and add training samples one-by-one
disp('Loading data...')
[imgFiles, ptsFiles] = cv.Facemark.loadDatasetList(imgList, ptsList);
for i=1:numel(imgFiles)
    % load image and its corresponding annotation data, then add pair
    img = cv.imread(imgFiles{i});
    pts = cv.Facemark.loadFacePoints(ptsFiles{i});
    obj.addTrainingSample(img, pts);
end

%% Train
% train the algorithm, model will be saved to specified file
disp('Training...')
tic
obj.training();
toc

%% Prepare for Test
% Since the AAM algorithm needs initialization parameters (rotation,
% translation, and scaling), we need to declare the required variable to store
% these information which will be obtained using a custom function. The
% implementation of |getInitialFitting| function in this example is not
% optimal, you can always create your own function.
%
% The initialization is obtained by comparing the base shape of the trained
% model with the current face image. In this case, the rotation is obtained by
% comparing the angle of line formed by two eyes in the input face image with
% the same line in the base shape. Meanwhile, the scaling is obtained by
% comparing the length of line between eyes in the input image compared to the
% base shape.
%
% The fitting process starts by detecting faces in given image.
%
% If at least one face is found, then the next step is computing the
% initialization parameters. In this case, since |getInitialFitting| function
% is not optimal, it may not find pair of eyes from a given face. Therefore,
% we will filter out faces without initialization parameters and in this case,
% each element in the |confs| vector represent the initialization parameters
% for each filtered face.

%%
% create cascade detector objects (for face and eyes)
ccFace = cv.CascadeClassifier(xmlFace);
ccEyes = cv.CascadeClassifier(xmlEyes);

%%
% detect faces
img = cv.imread(testImg);
faces = myFaceDetector(img, ccFace);
assert(~isempty(faces), 'no faces found');
fprintf('%d faces\n', numel(faces));

%%
% get base shape from trained model
s0 = obj.getData();
s0 = cat(1, s0{:});

%%
% compute initialization params for each detected face
S = struct('R',eye(2), 't',[0 0], 'scale',1);
confs = S([]);
faces_eyes = {};
for i=1:numel(faces)
    [conf, found] = getInitialFitting(img, faces{i}, s0, ccEyes);
    if found
        confs(end+1) = conf;
        faces_eyes{end+1} = faces{i};
    end
end
assert(~isempty(confs), 'failed to compute initialization params');
fprintf('%d faces with eyes\n', numel(confs));

%%
% For the fitting parameters stored in the |confs| vector, |scaleIdx| field
% represents the ID of scaling factor that will be used in the fitting process.
% In this example the fitting will use the biggest scaling factor (4) which is
% expected to have the fastest computation time compared to the other scales.
% If the ID is bigger than the available trained scales in the model, the
% model with the biggest scale ID is used.
confs.scaleIdx = numel(scales) - 1;

%% Test
% The fitting process is quite simple, you just need to pass the image, array
% of rectangles representing the ROIs of all faces in the given image, and the
% configuration params. It returns the landmark points.
tic
landmarks = obj.fit(img, faces_eyes, 'Configs',confs);
toc

%%
% After the fitting process is finished, we can visualize the result
for i=1:numel(landmarks)
    img = cv.Facemark.drawFacemarks(img, landmarks{i}, 'Color',[0 255 0]);
end
imshow(img)

%% Helper functions

function download_classifier_xml(fname)
    if exist(fname, 'file') ~= 2
        % attempt to download trained Haar/LBP/HOG classifier from Github
        url = 'https://cdn.rawgit.com/opencv/opencv/3.4.0/data/';
        [~, f, ext] = fileparts(fname);
        if strncmpi(f, 'haarcascade_', length('haarcascade_'))
            url = [url, 'haarcascades/'];
        elseif strncmpi(f, 'lbpcascade_', length('lbpcascade_'))
            url = [url, 'lbpcascades/'];
        elseif strncmpi(f, 'hogcascade_', length('hogcascade_'))
            url = [url, 'hogcascades/'];
        else
            error('File not found');
        end
        urlwrite([url f ext], fname);
    end
end

function faces = myFaceDetector(img, ccFace)
    %MYFACEDETECTOR  Detect faces
    %
    %    faces = myFaceDetector(img, ccFace)
    %
    % ## Input
    % * __img__ input image
    % * __ccFace__ cascade object for face detection
    %
    % ## Output
    % * __faces__ detected faces, `{[x,y,w,h], ...}`
    %
    % See also: cv.Facemark.getFacesHAAR
    %

    if size(img,3) > 1
        gray = cv.cvtColor(img, 'RGB2GRAY');
    else
        gray = img;
    end
    gray = cv.equalizeHist(gray);
    faces = ccFace.detect(gray, 'ScaleFactor',1.4, 'MinNeighbors',2, ...
        'ScaleImage',true, 'MinSize',[30 30]);
end

function [conf, found] = getInitialFitting(img, face, s0, ccEyes)
    %GETINITIALFITTING  Calculate AAM intial fit params
    %
    %     [conf, found] = getInitialFitting(img, face, s0, ccEyes)
    %
    % ## Input
    % * __img__ input image
    % * __face__ detected face `[x,y,w,h]`
    % * __s0__ base shape of the trained model
    % * __ccEyes__ cascade object for eyes detection
    %
    % ## Output
    % * __conf__ struct with rotation, translation, and scale
    % * __found__ success flag
    %

    found = false;
    conf = struct('R',eye(2), 't',[0 0], 'scale',1.0);

    % detect eyes in face
    if cv.Rect.area(face) == 0, return; end
    faceROI = cv.Rect.crop(img, face);
    eyes = ccEyes.detect(faceROI, 'ScaleFactor',1.1, 'MinNeighbors',2, ...
        'ScaleImage',true, 'MinSize',[20 20]);
    if numel(eyes) ~= 2, return; end

    % make sure that first is left eye, second is right eye
    if eyes{2}(1) < eyes{1}(1)
        eyes = eyes([2 1]);
    end

    % eyes centers in detected face
    c1 = face(1:2) + eyes{1}(1:2) + eyes{1}(3:4)/2;  % left eye
    c2 = face(1:2) + eyes{2}(1:2) + eyes{2}(3:4)/2;  % right eye
    assert(c1(1) < c2(1), 'eyes not ordered correctly (left then right)');

    % eyes centers in base shape (shifted to middle of image)
    base = bsxfun(@plus, s0, [size(img,2) size(img,1)]/2);
    c1Base = (base(37,:) + base(40,:)) / 2;  % left eye
    c2Base = (base(43,:) + base(46,:)) / 2;  % right eye

    % scale between the two line length in detected and base shape
    scale = norm(c2 - c1) / norm(c2Base - c1Base);

    % eyes centers in scaled base shape (not shifted)
    base = s0 * scale;
    c1Base = (base(37,:) + base(40,:)) / 2;
    c2Base = (base(43,:) + base(46,:)) / 2;

    % angle of horizontal line connecting eyes centers in scaled base shape
    aBase = atan2(c2Base(2) - c1Base(2), c2Base(1) - c1Base(1));

    % angle of horizontal line connecting eyes centers in detect face
    a = atan2(c2(2) - c1(2), c2(1) - c1(1));

    % rotation matrix from the two angles
    R = cv.getRotationMatrix2D([0 0], rad2deg(aBase-a), 1.0);
    R = R(1:2,1:2);

    % eyes centers in transformed base shape (scaled then rotated)
    base = (R * scale * s0')';
    c1Base = (base(37,:) + base(40,:)) / 2;
    c2Base = (base(43,:) + base(46,:)) / 2;

    % translation between detected and transformed base shape
    t = c1 - c1Base;

    % fill output
    found = true;
    conf.R = R;
    conf.t = t;
    conf.scale = scale;
end

##### SOURCE END #####
-->
   </body>
</html>