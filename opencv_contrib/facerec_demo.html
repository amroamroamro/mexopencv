<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>Face recognition demo</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2017-11-27">
      <meta name="DC.source" content="facerec_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">Face recognition demo</h1>
         <!--introduction-->
         <p>Demonstration of face recognition with OpenCV.</p>
         <p>The currently available algorithms are:</p>
         <div>
            <ul>
               <li>Eigenfaces</li>
               <li>Fisherfaces</li>
               <li>Local Binary Patterns Histograms</li>
            </ul>
         </div>
         <p>See this page for a complete tutorial: <a href="https://docs.opencv.org/3.1.0/da/d60/tutorial_face_main.html">https://docs.opencv.org/3.1.0/da/d60/tutorial_face_main.html</a></p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_demo.cpp">https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_demo.cpp</a></li>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_eigenfaces.cpp">https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_eigenfaces.cpp</a></li>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_fisherfaces.cpp">https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_fisherfaces.cpp</a></li>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_lbph.cpp">https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_lbph.cpp</a></li>
            </ul>
         </div>
         <!--/introduction-->
         <h2 id="toc">Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Options</a></li>
               <li><a href="#3">Database</a></li>
               <li><a href="#9">Create face recognizer</a></li>
               <li><a href="#11">Train</a></li>
               <li><a href="#13">Predict</a></li>
               <li><a href="#14">Evaluation</a></li>
               <li><a href="#15">Visualize results</a></li>
            </ul>
         </div>
         <h2 id="2">Options</h2><pre class="codeinput"><span class="comment">% face recognizer type</span>
recognizer = <span class="string">'Eigenfaces'</span>;  <span class="comment">% Eigenfaces, Fisherfaces, LBPH</span>

<span class="comment">% Eigenfaces: this performs a full PCA, if you just want to keep 10 principal</span>
<span class="comment">% components (Eigenfaces), then set it to 10</span>
<span class="comment">% Fisherfaces: If you just want to keep 10 Fisherfaces, then set it to 10</span>
<span class="comment">% However it is not useful to discard Fisherfaces! Please always try to use</span>
<span class="comment">% all available Fisherfaces for classification.</span>
numComp = 0;

<span class="comment">% if you want to set a confidennce threshold, set it to e.g. 123.0</span>
thresh = realmax;

<span class="comment">% test set split ratio</span>
P = 0.1;</pre><h2 id="3">Database</h2>
         <p>In this example, we use the AT&amp;T Facedatabase, sometimes also referred to as ORL Database of Faces, contains ten different
            images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting,
            facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were
            taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side
            movement).
         </p>
         <p>The files are in PGM format. The size of each image is 92x112 pixels, with 256 grey levels per pixel. The images are organised
            in 40 directories (one for each subject), which have names of the form sX, where X indicates the subject number (between 1
            and 40). In each of these directories, there are ten different images of that subject, which have names of the form Y.pgm,
            where Y is the image number for that subject (between 1 and 10).
         </p>
         <p>A copy of the database can be retrieved from: <a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html</a></p>
         <p>download/extract files if needed</p><pre class="codeinput">ATTFolder = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'att_faces'</span>);
<span class="keyword">if</span> ~isdir(ATTFolder)
    mkdir(ATTFolder);
    zipFile = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'att_faces.zip'</span>);
    <span class="keyword">if</span> exist(zipFile, <span class="string">'file'</span>) ~= 2
        url = <span class="string">'http://www.cl.cam.ac.uk/Research/DTG/attarchive/pub/data/att_faces.zip'</span>;
        disp(<span class="string">'Downloading AT&amp;T Faces...'</span>);
        urlwrite(url, zipFile);
    <span class="keyword">end</span>
    disp(<span class="string">'Extracting files...'</span>);
    unzip(zipFile, ATTFolder);
<span class="keyword">end</span></pre><p>read all images (grayscale) and record corresponding labels</p><pre class="codeinput">folders = dir(fullfile(ATTFolder, <span class="string">'s*'</span>));
NL = numel(folders);
w = -1; h = -1;  <span class="comment">% image width/height</span>

fprintf(<span class="string">'Loading images... '</span>)
labels = [];
images = {};
<span class="keyword">for</span> i=1:NL
    files = dir(fullfile(ATTFolder, folders(i).name, <span class="string">'*.pgm'</span>));
    <span class="keyword">for</span> j=1:numel(files)
        fname = fullfile(ATTFolder, folders(i).name, files(j).name);
        img = cv.imread(fname, <span class="string">'Grayscale'</span>,true);
        labels(end+1) = i;
        images{end+1} = img;
        <span class="keyword">if</span> w&lt;0 || h&lt;0
            [h,w,~] = size(img);
        <span class="keyword">else</span>
            assert(h==size(img,1) &amp;&amp; w==size(img,2), <span class="keyword">...</span>
                <span class="string">'Images should be of the same size!'</span>);
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
fprintf(<span class="string">'Done\n'</span>);

assert(numel(images) &gt; 1, <span class="string">'This demo needs at least 2 images to work.'</span>);
<span class="keyword">if</span> h&lt;50 || w&lt;50
    disp(<span class="string">'For better results images should be not smaller than 50x50!'</span>);
<span class="keyword">end</span></pre><pre class="codeoutput">Loading images... Done
</pre><p>show the full dataset (40 people with 10 views each)</p><pre class="codeinput"><span class="keyword">if</span> mexopencv.require(<span class="string">'images'</span>)
    <span class="comment">% each column is a different person, with different views across rows</span>
    im = reshape(images,[10 40])';
    montage(cat(4,im{:}), <span class="string">'Size'</span>,[10 40]);
    clear <span class="string">im</span>
<span class="keyword">end</span></pre><pre class="codeoutput">Warning: Image is too big to fit on screen; displaying at 33% 
</pre><img src="facerec_demo_01.png"><p>helper function to reshape and normalize a feature vector for visualization</p><pre class="codeinput">vec2img = @(vec) cv.normalize(reshape(vec, [w h]).', <span class="keyword">...</span>
    <span class="string">'Alpha'</span>,0, <span class="string">'Beta'</span>,255, <span class="string">'NormType'</span>,<span class="string">'MinMax'</span>, <span class="string">'DType'</span>,<span class="string">'uint8'</span>);
img2vec = @(img) reshape(img.', 1, []);</pre><p>split the images into train/test sets by holding out a part for testing (this is done so that the train and test data do not
            overlap)
         </p><pre class="codeinput">C = cvpartition(labels, <span class="string">'HoldOut'</span>,P)
<span class="keyword">if</span> mexopencv.isOctave()
    <span class="comment">%HACK: Octave's CVPARTITION implemented using old class OOP (pre classdef)</span>
    <span class="comment">% which doesn't expose properties but accessor methods</span>
    CTestSize = get(C, <span class="string">'TestSize'</span>);
<span class="keyword">else</span>
    CTestSize = C.TestSize;
<span class="keyword">end</span>
<span class="comment">%tabulate(labels(test(C)))</span></pre><pre class="codeoutput">ans = 
Hold-out cross validation partition
   NumObservations: 400
       NumTestSets: 1
         TrainSize: 360
          TestSize: 40
</pre><h2 id="9">Create face recognizer</h2>
         <p>create an model for face recognition</p><pre class="codeinput"><span class="keyword">if</span> strcmp(recognizer, <span class="string">'LBPH'</span>)
    model = cv.LBPHFaceRecognizer(<span class="string">'Threshold'</span>,thresh);
<span class="keyword">else</span>
    model = cv.BasicFaceRecognizer(recognizer, <span class="keyword">...</span>
        <span class="string">'NumComponents'</span>,numComp, <span class="string">'Threshold'</span>,thresh);
<span class="keyword">end</span>
display(model);
disp(model.typeid())</pre><pre class="codeoutput">model = 
  BasicFaceRecognizer with properties:

               id: 1
    NumComponents: 0
        Threshold: 1.7977e+308
class cv::face::Eigenfaces
</pre><p>set labels info (person name corresponding to each class label)</p><pre class="codeinput"><span class="comment">%labelsInfo = containers.Map(1:NL, {folders.name});</span>
<span class="keyword">for</span> i=1:NL
    model.setLabelInfo(i, folders(i).name);
<span class="keyword">end</span></pre><h2 id="11">Train</h2>
         <p>train model with images/labels read</p><pre class="codeinput">fprintf(<span class="string">'Training... '</span>); tic
model.train(images(training(C)), labels(training(C)));
toc</pre><pre class="codeoutput">Training... Elapsed time is 5.695211 seconds.
</pre><p>save/load trained model</p><pre class="codeinput">saveModelPath = <span class="string">'face-rec-model.yml'</span>;
<span class="keyword">if</span> false
    model.save(saveModelPath);
    model.load(saveModelPath);
<span class="keyword">end</span></pre><h2 id="13">Predict</h2>
         <p>predict the label of given test images</p><pre class="codeinput">fprintf(<span class="string">'Predicting... '</span>); tic
labelsHat = zeros(1, CTestSize);
confidences = zeros(1, CTestSize);
testInd = find(test(C));  <span class="comment">% indices of test images</span>
<span class="keyword">for</span> i=1:CTestSize
    [labelsHat(i), confidences(i)] = model.predict(images{testInd(i)});
<span class="keyword">end</span>
toc</pre><pre class="codeoutput">Predicting... Elapsed time is 0.378952 seconds.
</pre><h2 id="14">Evaluation</h2><pre class="codeinput">acc = nnz(labels(test(C)) == labelsHat) ./ CTestSize;
fprintf(<span class="string">'Accuracy = %.2f%%\n'</span>, acc*100);
<span class="keyword">if</span> false
    cm = confusionmat(labels(test(C)), labelsHat);
    disp(<span class="string">'Confusion Matrix:'</span>); disp(cm);
<span class="keyword">end</span>

disp(<span class="string">'Misclassifications:'</span>);
ind = find(labels(test(C)) ~= labelsHat);  <span class="comment">% indices of misclassification</span>
<span class="keyword">for</span> i=1:numel(ind)
    fprintf(<span class="string">'  Predicted = %2d (%3s), Actual = %2d (%3s) [Conf/Dist = %g]\n'</span>, <span class="keyword">...</span>
        labelsHat(ind(i)), model.getLabelInfo(labelsHat(ind(i))), <span class="keyword">...</span>
        labels(testInd(ind(i))), model.getLabelInfo(labels(testInd(ind(i)))), <span class="keyword">...</span>
        confidences(ind(i)));
<span class="keyword">end</span></pre><pre class="codeoutput">Accuracy = 97.50%
Misclassifications:
  Predicted =  7 (s15), Actual = 11 (s19) [Conf/Dist = 3759.37]
</pre><h2 id="15">Visualize results</h2><pre class="codeinput"><span class="keyword">if</span> strcmp(recognizer, <span class="string">'LBPH'</span>)
    <span class="comment">% There's no cool data to show as in Eigen/Fisher faces. Due to</span>
    <span class="comment">% efficiency reasons the LBP images are not stored within the model.</span>
    <span class="comment">% We could perhaps visualize the histograms?</span>
    histograms = model.getHistograms();
    fprintf(<span class="string">'Size of the histograms: %d\n'</span>, numel(histograms{1}));
<span class="keyword">else</span>
    <span class="comment">% get the mean, eigenvectors and eigenvalues of the training data</span>
    <span class="comment">%  X{i} : 1-by-(w*h)</span>
    <span class="comment">%  M    : 1-by-(w*h)</span>
    <span class="comment">%  W    : (w*h)-by-NumComponents</span>
    <span class="comment">%  EV   : 1-by-NumComponents</span>
    <span class="comment">%  Y{i} : 1-by-NumComponents</span>
    M = model.getMean();
    W = model.getEigenVectors();
    EV = model.getEigenValues();
    <span class="comment">%Y = model.getProjections();</span>

    <span class="comment">% collect the first 16 eigenfaces/fisherfaces</span>
    out = cell(1, min(16, model.NumComponents));
    <span class="keyword">for</span> i=1:numel(out)
        <span class="comment">% i-th eigenvector: reshaped to image size and normalized to [0,255]</span>
        gray = vec2img(W(:,i));
        <span class="comment">% apply a colormap for better sensing</span>
        out{i} = cv.applyColorMap(gray, <span class="string">'Bone'</span>);
    <span class="keyword">end</span>

    figure(2)
    subplot(1,4,1), imshow(vec2img(M)), title(<span class="string">'mean face'</span>)
    subplot(1,4,2:4), montage(cat(4,out{:}), <span class="string">'Size'</span>,[2 NaN])
    title(sprintf(<span class="string">'First %d %s'</span>, numel(out), recognizer))

    <span class="comment">% image reconstruction at some predefined steps</span>
    <span class="keyword">if</span> strcmp(recognizer, <span class="string">'Eigenfaces'</span>)
        steps = round(linspace(5, min(300,model.NumComponents), 20));
    <span class="keyword">else</span>
        steps = 1:min(16,model.NumComponents);
    <span class="keyword">end</span>
    display(steps)

    <span class="comment">% pick an image at random, and compute its successive reconstructions</span>
    X = img2vec(images{randi(numel(images))});
    out = cell(1, numel(steps));
    <span class="keyword">for</span> i=1:numel(steps)
        <span class="comment">% slice eigenvectors</span>
        <span class="keyword">if</span> strcmp(recognizer, <span class="string">'Eigenfaces'</span>)
            WW = W(:,1:steps(i));
        <span class="keyword">else</span>
            WW = W(:,steps(i));
        <span class="keyword">end</span>
        <span class="comment">% project and reconstruct using truncated eigenvectors</span>
        <span class="comment">% Y = (X-M)*W, XX = Y*W'+M</span>
        projection = cv.LDA.subspaceProject(WW, M, X);
        reconstruction = cv.LDA.subspaceReconstruct(WW, M, projection);
        <span class="comment">% normalize the result and reshape it to image size</span>
        out{i} = vec2img(reconstruction);
    <span class="keyword">end</span>

    figure(3)
    subplot(4,1,1:3), montage(cat(4,out{:}), <span class="string">'Size'</span>,[3 NaN])
    title(<span class="string">'Reconstructions'</span>)
    prcnt = 100 * cumsum(EV) ./ sum(EV);
    subplot(4,1,4), plot(1:model.NumComponents, prcnt, <span class="string">'-'</span>, <span class="keyword">...</span>
        steps, prcnt(steps), <span class="string">'o'</span>)
    axis([1 model.NumComponents 0 100]), grid <span class="string">on</span>
    title(<span class="string">'Percentage Explained (%)'</span>), xlabel(<span class="string">'NumComponents'</span>)
<span class="keyword">end</span></pre><pre class="codeoutput">steps =
  Columns 1 through 13
     5    21    36    52    67    83    98   114   129   145   160   176   191
  Columns 14 through 20
   207   222   238   253   269   284   300
</pre><img src="facerec_demo_02.png"><img src="facerec_demo_03.png"><div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Face recognition demo
% Demonstration of face recognition with OpenCV.
%
% The currently available algorithms are:
%
% * Eigenfaces
% * Fisherfaces
% * Local Binary Patterns Histograms
%
% See this page for a complete tutorial:
% <https://docs.opencv.org/3.1.0/da/d60/tutorial_face_main.html>
%
% Sources:
%
% * <https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_demo.cpp>
% * <https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_eigenfaces.cpp>
% * <https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_fisherfaces.cpp>
% * <https://github.com/opencv/opencv_contrib/blob/3.1.0/modules/face/samples/cpp/facerec_lbph.cpp>
%

%% Options

% face recognizer type
recognizer = 'Eigenfaces';  % Eigenfaces, Fisherfaces, LBPH

% Eigenfaces: this performs a full PCA, if you just want to keep 10 principal
% components (Eigenfaces), then set it to 10
% Fisherfaces: If you just want to keep 10 Fisherfaces, then set it to 10
% However it is not useful to discard Fisherfaces! Please always try to use
% all available Fisherfaces for classification.
numComp = 0;

% if you want to set a confidennce threshold, set it to e.g. 123.0
thresh = realmax;

% test set split ratio
P = 0.1;

%% Database
%
% In this example, we use the AT&T Facedatabase, sometimes also referred to as
% ORL Database of Faces, contains ten different images of each of 40 distinct
% subjects. For some subjects, the images were taken at different times,
% varying the lighting, facial expressions (open / closed eyes, smiling / not
% smiling) and facial details (glasses / no glasses). All the images were
% taken against a dark homogeneous background with the subjects in an upright,
% frontal position (with tolerance for some side movement).
%
% The files are in PGM format. The size of each image is 92x112 pixels, with
% 256 grey levels per pixel. The images are organised in 40 directories (one
% for each subject), which have names of the form sX, where X indicates the
% subject number (between 1 and 40). In each of these directories, there are
% ten different images of that subject, which have names of the form Y.pgm,
% where Y is the image number for that subject (between 1 and 10).
%
% A copy of the database can be retrieved from:
% <http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html>
%

%%
% download/extract files if needed
ATTFolder = fullfile(mexopencv.root(),'test','att_faces');
if ~isdir(ATTFolder)
    mkdir(ATTFolder);
    zipFile = fullfile(mexopencv.root(),'test','att_faces.zip');
    if exist(zipFile, 'file') ~= 2
        url = 'http://www.cl.cam.ac.uk/Research/DTG/attarchive/pub/data/att_faces.zip';
        disp('Downloading AT&T Faces...');
        urlwrite(url, zipFile);
    end
    disp('Extracting files...');
    unzip(zipFile, ATTFolder);
end

%%
% read all images (grayscale) and record corresponding labels
folders = dir(fullfile(ATTFolder, 's*'));
NL = numel(folders);
w = -1; h = -1;  % image width/height

fprintf('Loading images... ')
labels = [];
images = {};
for i=1:NL
    files = dir(fullfile(ATTFolder, folders(i).name, '*.pgm'));
    for j=1:numel(files)
        fname = fullfile(ATTFolder, folders(i).name, files(j).name);
        img = cv.imread(fname, 'Grayscale',true);
        labels(end+1) = i;
        images{end+1} = img;
        if w<0 || h<0
            [h,w,~] = size(img);
        else
            assert(h==size(img,1) && w==size(img,2), ...
                'Images should be of the same size!');
        end
    end
end
fprintf('Done\n');

assert(numel(images) > 1, 'This demo needs at least 2 images to work.');
if h<50 || w<50
    disp('For better results images should be not smaller than 50x50!');
end

%%
% show the full dataset (40 people with 10 views each)
if mexopencv.require('images')
    % each column is a different person, with different views across rows
    im = reshape(images,[10 40])';
    montage(cat(4,im{:}), 'Size',[10 40]);
    clear im
end

%%
% helper function to reshape and normalize a feature vector for visualization
vec2img = @(vec) cv.normalize(reshape(vec, [w h]).', ...
    'Alpha',0, 'Beta',255, 'NormType','MinMax', 'DType','uint8');
img2vec = @(img) reshape(img.', 1, []);

%%
% split the images into train/test sets by holding out a part for testing
% (this is done so that the train and test data do not overlap)
C = cvpartition(labels, 'HoldOut',P)
if mexopencv.isOctave()
    %HACK: Octave's CVPARTITION implemented using old class OOP (pre classdef)
    % which doesn't expose properties but accessor methods
    CTestSize = get(C, 'TestSize');
else
    CTestSize = C.TestSize;
end
%tabulate(labels(test(C)))

%% Create face recognizer
% create an model for face recognition
if strcmp(recognizer, 'LBPH')
    model = cv.LBPHFaceRecognizer('Threshold',thresh);
else
    model = cv.BasicFaceRecognizer(recognizer, ...
        'NumComponents',numComp, 'Threshold',thresh);
end
display(model);
disp(model.typeid())

%%
% set labels info (person name corresponding to each class label)
%labelsInfo = containers.Map(1:NL, {folders.name});
for i=1:NL
    model.setLabelInfo(i, folders(i).name);
end

%% Train
% train model with images/labels read
fprintf('Training... '); tic
model.train(images(training(C)), labels(training(C)));
toc

%%
% save/load trained model
saveModelPath = 'face-rec-model.yml';
if false
    model.save(saveModelPath);
    model.load(saveModelPath);
end

%% Predict
% predict the label of given test images
fprintf('Predicting... '); tic
labelsHat = zeros(1, CTestSize);
confidences = zeros(1, CTestSize);
testInd = find(test(C));  % indices of test images
for i=1:CTestSize
    [labelsHat(i), confidences(i)] = model.predict(images{testInd(i)});
end
toc

%% Evaluation
acc = nnz(labels(test(C)) == labelsHat) ./ CTestSize;
fprintf('Accuracy = %.2f%%\n', acc*100);
if false
    cm = confusionmat(labels(test(C)), labelsHat);
    disp('Confusion Matrix:'); disp(cm);
end

disp('Misclassifications:');
ind = find(labels(test(C)) ~= labelsHat);  % indices of misclassification
for i=1:numel(ind)
    fprintf('  Predicted = %2d (%3s), Actual = %2d (%3s) [Conf/Dist = %g]\n', ...
        labelsHat(ind(i)), model.getLabelInfo(labelsHat(ind(i))), ...
        labels(testInd(ind(i))), model.getLabelInfo(labels(testInd(ind(i)))), ...
        confidences(ind(i)));
end

%% Visualize results
if strcmp(recognizer, 'LBPH')
    % There's no cool data to show as in Eigen/Fisher faces. Due to
    % efficiency reasons the LBP images are not stored within the model.
    % We could perhaps visualize the histograms?
    histograms = model.getHistograms();
    fprintf('Size of the histograms: %d\n', numel(histograms{1}));
else
    % get the mean, eigenvectors and eigenvalues of the training data
    %  X{i} : 1-by-(w*h)
    %  M    : 1-by-(w*h)
    %  W    : (w*h)-by-NumComponents
    %  EV   : 1-by-NumComponents
    %  Y{i} : 1-by-NumComponents
    M = model.getMean();
    W = model.getEigenVectors();
    EV = model.getEigenValues();
    %Y = model.getProjections();

    % collect the first 16 eigenfaces/fisherfaces
    out = cell(1, min(16, model.NumComponents));
    for i=1:numel(out)
        % i-th eigenvector: reshaped to image size and normalized to [0,255]
        gray = vec2img(W(:,i));
        % apply a colormap for better sensing
        out{i} = cv.applyColorMap(gray, 'Bone');
    end

    figure(2)
    subplot(1,4,1), imshow(vec2img(M)), title('mean face')
    subplot(1,4,2:4), montage(cat(4,out{:}), 'Size',[2 NaN])
    title(sprintf('First %d %s', numel(out), recognizer))

    % image reconstruction at some predefined steps
    if strcmp(recognizer, 'Eigenfaces')
        steps = round(linspace(5, min(300,model.NumComponents), 20));
    else
        steps = 1:min(16,model.NumComponents);
    end
    display(steps)

    % pick an image at random, and compute its successive reconstructions
    X = img2vec(images{randi(numel(images))});
    out = cell(1, numel(steps));
    for i=1:numel(steps)
        % slice eigenvectors
        if strcmp(recognizer, 'Eigenfaces')
            WW = W(:,1:steps(i));
        else
            WW = W(:,steps(i));
        end
        % project and reconstruct using truncated eigenvectors
        % Y = (X-M)*W, XX = Y*W'+M
        projection = cv.LDA.subspaceProject(WW, M, X);
        reconstruction = cv.LDA.subspaceReconstruct(WW, M, projection);
        % normalize the result and reshape it to image size
        out{i} = vec2img(reconstruction);
    end

    figure(3)
    subplot(4,1,1:3), montage(cat(4,out{:}), 'Size',[3 NaN])
    title('Reconstructions')
    prcnt = 100 * cumsum(EV) ./ sum(EV);
    subplot(4,1,4), plot(1:model.NumComponents, prcnt, '-', ...
        steps, prcnt(steps), 'o')
    axis([1 model.NumComponents 0 100]), grid on
    title('Percentage Explained (%)'), xlabel('NumComponents')
end

##### SOURCE END #####
-->
   </body>
</html>