<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>Facemark LBF training demo</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2018-02-21">
      <meta name="DC.source" content="facemark_lbf_train_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">Facemark LBF training demo</h1>
         <!--introduction-->
         <p>The user should provides the list of training images accompanied by their corresponding landmarks location in separate files.</p>
         <p>See below for a description of file formats.</p>
         <p>Examples of datasets are available at <a href="https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/">https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/</a>.
         </p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/samples/facemark_demo_lbf.cpp">https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/samples/facemark_demo_lbf.cpp</a></li>
               <li><a href="https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/tutorials/facemark_usage/facemark_usage.markdown">https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/tutorials/facemark_usage/facemark_usage.markdown</a></li>
            </ul>
         </div>
         <!--/introduction-->
         <h2 id="toc">Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Preparation</a></li>
               <li><a href="#3">Options</a></li>
               <li><a href="#4">Init</a></li>
               <li><a href="#6">Data</a></li>
               <li><a href="#7">Train</a></li>
               <li><a href="#8">Test</a></li>
               <li><a href="#9">Helper functions</a></li>
            </ul>
         </div>
         <h2 id="2">Preparation</h2>
         <p>Before you continue with this tutorial, you should download a training dataset of facial landmarks detection.</p>
         <p>We suggest you to download the IBUG dataset which can be retrieved at <a href="https://ibug.doc.ic.ac.uk/download/annotations/ibug.zip">https://ibug.doc.ic.ac.uk/download/annotations/ibug.zip</a></p>
         <p>First thing to do is to make two text files containing the list of image files and annotation files respectively. Make sure
            that the order of images and annotations in both files are matched. Furthermore, it is advised to use absolute paths instead
            of relative paths.
         </p>
         <p>Example to make the file list in Linux machine:</p><pre>ls /data/ibug/*.jpg &gt; images.txt
ls /data/ibug/*.pts &gt; annotations.txt</pre><p>Example of content in the <tt>images.txt</tt> file:
         </p><pre>/data/ibug/image_003_1.jpg
/data/ibug/image_004_1.jpg
/data/ibug/image_005_1.jpg
...</pre><p>Example of content in the <tt>annotations.txt</tt> file:
         </p><pre>/data/ibug/image_003_1.pts
/data/ibug/image_004_1.pts
/data/ibug/image_005_1.pts
...</pre><p>where a <tt>.pts</tt> file contains the position of each face landmark. Make sure that the annotation format is supported by the API, where the
            contents should look like the following snippet:
         </p><pre>version: 1
n_points:  68
{
212.716603 499.771793
230.232816 566.290071
...
}</pre><p>Once trained, we show how to use the model to detect face landmarks in a test image.</p>
         <p>You can also download a pre-trained model in this link <a href="https://raw.githubusercontent.com/kurnianggoro/GSOC2017/master/data/lbfmodel.yaml">https://raw.githubusercontent.com/kurnianggoro/GSOC2017/master/data/lbfmodel.yaml</a> (that way you can skip training and simply load the model).
         </p>
         <h2 id="3">Options</h2><pre class="codeinput"><span class="comment">% [INPUT] path of a text file contains the list of paths to all training images</span>
imgList = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'facemark'</span>,<span class="string">'lfpw'</span>,<span class="string">'images.lst'</span>);
assert(exist(imgList, <span class="string">'file'</span>) == 2, <span class="string">'missing images list file'</span>);

<span class="comment">% [INPUT] path of a text file contains the list of paths to all annotations files</span>
ptsList = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'facemark'</span>,<span class="string">'lfpw'</span>,<span class="string">'annotations.lst'</span>);
assert(exist(ptsList, <span class="string">'file'</span>) == 2, <span class="string">'missing annotations list file'</span>);

<span class="comment">% [OUTPUT] path for saving the trained model</span>
modelFile = fullfile(tempdir(), <span class="string">'model_lbf.yaml'</span>);

<span class="comment">% [INPUT] path to the cascade xml file for the face detector</span>
xmlFace = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'lbpcascade_frontalface.xml'</span>);
download_classifier_xml(xmlFace);

<span class="comment">% name of user-defined face detector function</span>
faceDetectFcn = <span class="string">'myFaceDetector'</span>;
assert(exist([faceDetectFcn <span class="string">'.m'</span>], <span class="string">'file'</span>) == 2, <span class="string">'missing face detect function'</span>);

<span class="comment">% path to test image</span>
testImg = fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'lena.jpg'</span>);</pre><h2 id="4">Init</h2>
         <p>create the facemark instance</p><pre class="codeinput">obj = cv.Facemark(<span class="string">'LBF'</span>, <span class="string">'NLandmarks'</span>,68, <span class="string">'CascadeFace'</span>,xmlFace, <span class="keyword">...</span>
    <span class="string">'ModelFilename'</span>,modelFile, <span class="string">'SaveModel'</span>,true, <span class="string">'Verbose'</span>,true);</pre><p>set user-defined face detector</p><pre class="codeinput">obj.setFaceDetector(faceDetectFcn);</pre><h2 id="6">Data</h2>
         <p>load the dataset, and add training samples one-by-one</p><pre class="codeinput">disp(<span class="string">'Loading data...'</span>)
[imgFiles, ptsFiles] = cv.Facemark.loadDatasetList(imgList, ptsList);
<span class="keyword">for</span> i=1:numel(imgFiles)
    <span class="comment">% load image and its corresponding annotation data, then add pair</span>
    img = cv.imread(imgFiles{i});
    pts = cv.Facemark.loadFacePoints(ptsFiles{i});
    obj.addTrainingSample(img, pts);
<span class="keyword">end</span></pre><pre class="codeoutput">Loading data...
</pre><h2 id="7">Train</h2>
         <p>train the algorithm, model will be saved to specified file</p><pre class="codeinput">disp(<span class="string">'Training...'</span>)
tic
obj.training();
toc</pre><pre class="codeoutput">Training...
Elapsed time is 107.133092 seconds.
</pre><h2 id="8">Test</h2>
         <p>run on some test image</p><pre class="codeinput">img = cv.imread(testImg);
faces = obj.getFaces(img);
landmarks = obj.fit(img, faces);
<span class="keyword">for</span> i=1:numel(faces)
    img = cv.rectangle(img, faces{i}, <span class="string">'Color'</span>,[255 0 255]);
    img = cv.Facemark.drawFacemarks(img, landmarks{i}, <span class="string">'Color'</span>,[0 0 255]);
<span class="keyword">end</span>
imshow(img)</pre><img src="facemark_lbf_train_demo_01.png"><h2 id="9">Helper functions</h2><pre class="codeinput"><span class="keyword">function</span> download_classifier_xml(fname)
    <span class="keyword">if</span> exist(fname, <span class="string">'file'</span>) ~= 2
        <span class="comment">% attempt to download trained Haar/LBP/HOG classifier from Github</span>
        url = <span class="string">'https://cdn.rawgit.com/opencv/opencv/3.4.0/data/'</span>;
        [~, f, ext] = fileparts(fname);
        <span class="keyword">if</span> strncmpi(f, <span class="string">'haarcascade_'</span>, length(<span class="string">'haarcascade_'</span>))
            url = [url, <span class="string">'haarcascades/'</span>];
        <span class="keyword">elseif</span> strncmpi(f, <span class="string">'lbpcascade_'</span>, length(<span class="string">'lbpcascade_'</span>))
            url = [url, <span class="string">'lbpcascades/'</span>];
        <span class="keyword">elseif</span> strncmpi(f, <span class="string">'hogcascade_'</span>, length(<span class="string">'hogcascade_'</span>))
            url = [url, <span class="string">'hogcascades/'</span>];
        <span class="keyword">else</span>
            error(<span class="string">'File not found'</span>);
        <span class="keyword">end</span>
        urlwrite([url f ext], fname);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">% The facemark API provides the functionality to the user to use their own</span>
<span class="comment">% face detector. The code below implements a sample face detector. This</span>
<span class="comment">% function must be saved in its own M-function to be used by the facemark API.</span>
<span class="keyword">function</span> faces = myFaceDetector(img)
    <span class="keyword">persistent</span> obj
    <span class="keyword">if</span> isempty(obj)
        obj = cv.CascadeClassifier();
        obj.load(xmlFace);
    <span class="keyword">end</span>

    <span class="keyword">if</span> size(img,3) &gt; 1
        gray = cv.cvtColor(img, <span class="string">'RGB2GRAY'</span>);
    <span class="keyword">else</span>
        gray = img;
    <span class="keyword">end</span>
    gray = cv.equalizeHist(gray);
    faces = obj.detect(gray, <span class="string">'ScaleFactor'</span>,1.4, <span class="string">'MinNeighbors'</span>,2, <span class="keyword">...</span>
        <span class="string">'ScaleImage'</span>,true, <span class="string">'MinSize'</span>,[30 30]);
<span class="keyword">end</span></pre><div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Facemark LBF training demo
%
% The user should provides the list of training images accompanied by their
% corresponding landmarks location in separate files.
%
% See below for a description of file formats.
%
% Examples of datasets are available at
% <https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/>.
%
% Sources:
%
% * <https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/samples/facemark_demo_lbf.cpp>
% * <https://github.com/opencv/opencv_contrib/blob/3.4.0/modules/face/tutorials/facemark_usage/facemark_usage.markdown>
%

%% Preparation
%
% Before you continue with this tutorial, you should download a training
% dataset of facial landmarks detection.
%
% We suggest you to download the IBUG dataset which can be retrieved at
% <https://ibug.doc.ic.ac.uk/download/annotations/ibug.zip>
%
% First thing to do is to make two text files containing the list of image
% files and annotation files respectively. Make sure that the order of images
% and annotations in both files are matched. Furthermore, it is advised to use
% absolute paths instead of relative paths.
%
% Example to make the file list in Linux machine:
%
%  ls /data/ibug/*.jpg > images.txt
%  ls /data/ibug/*.pts > annotations.txt
%
% Example of content in the |images.txt| file:
%
%  /data/ibug/image_003_1.jpg
%  /data/ibug/image_004_1.jpg
%  /data/ibug/image_005_1.jpg
%  ...
%
% Example of content in the |annotations.txt| file:
%
%  /data/ibug/image_003_1.pts
%  /data/ibug/image_004_1.pts
%  /data/ibug/image_005_1.pts
%  ...
%
% where a |.pts| file contains the position of each face landmark.
% Make sure that the annotation format is supported by the API, where the
% contents should look like the following snippet:
%
%  version: 1
%  n_points:  68
%  {
%  212.716603 499.771793
%  230.232816 566.290071
%  ...
%  }
%
% Once trained, we show how to use the model to detect face landmarks in a
% test image.
%
% You can also download a pre-trained model in this link
% <https://raw.githubusercontent.com/kurnianggoro/GSOC2017/master/data/lbfmodel.yaml>
% (that way you can skip training and simply load the model).
%

%% Options

% [INPUT] path of a text file contains the list of paths to all training images
imgList = fullfile(mexopencv.root(),'test','facemark','lfpw','images.lst');
assert(exist(imgList, 'file') == 2, 'missing images list file');

% [INPUT] path of a text file contains the list of paths to all annotations files
ptsList = fullfile(mexopencv.root(),'test','facemark','lfpw','annotations.lst');
assert(exist(ptsList, 'file') == 2, 'missing annotations list file');

% [OUTPUT] path for saving the trained model
modelFile = fullfile(tempdir(), 'model_lbf.yaml');

% [INPUT] path to the cascade xml file for the face detector
xmlFace = fullfile(mexopencv.root(),'test','lbpcascade_frontalface.xml');
download_classifier_xml(xmlFace);

% name of user-defined face detector function
faceDetectFcn = 'myFaceDetector';
assert(exist([faceDetectFcn '.m'], 'file') == 2, 'missing face detect function');

% path to test image
testImg = fullfile(mexopencv.root(),'test','lena.jpg');

%% Init
% create the facemark instance
obj = cv.Facemark('LBF', 'NLandmarks',68, 'CascadeFace',xmlFace, ...
    'ModelFilename',modelFile, 'SaveModel',true, 'Verbose',true);

%%
% set user-defined face detector
obj.setFaceDetector(faceDetectFcn);

%% Data
% load the dataset, and add training samples one-by-one
disp('Loading data...')
[imgFiles, ptsFiles] = cv.Facemark.loadDatasetList(imgList, ptsList);
for i=1:numel(imgFiles)
    % load image and its corresponding annotation data, then add pair
    img = cv.imread(imgFiles{i});
    pts = cv.Facemark.loadFacePoints(ptsFiles{i});
    obj.addTrainingSample(img, pts);
end

%% Train
% train the algorithm, model will be saved to specified file
disp('Training...')
tic
obj.training();
toc

%% Test
% run on some test image
img = cv.imread(testImg);
faces = obj.getFaces(img);
landmarks = obj.fit(img, faces);
for i=1:numel(faces)
    img = cv.rectangle(img, faces{i}, 'Color',[255 0 255]);
    img = cv.Facemark.drawFacemarks(img, landmarks{i}, 'Color',[0 0 255]);
end
imshow(img)

%% Helper functions

function download_classifier_xml(fname)
    if exist(fname, 'file') ~= 2
        % attempt to download trained Haar/LBP/HOG classifier from Github
        url = 'https://cdn.rawgit.com/opencv/opencv/3.4.0/data/';
        [~, f, ext] = fileparts(fname);
        if strncmpi(f, 'haarcascade_', length('haarcascade_'))
            url = [url, 'haarcascades/'];
        elseif strncmpi(f, 'lbpcascade_', length('lbpcascade_'))
            url = [url, 'lbpcascades/'];
        elseif strncmpi(f, 'hogcascade_', length('hogcascade_'))
            url = [url, 'hogcascades/'];
        else
            error('File not found');
        end
        urlwrite([url f ext], fname);
    end
end

% The facemark API provides the functionality to the user to use their own
% face detector. The code below implements a sample face detector. This
% function must be saved in its own M-function to be used by the facemark API.
function faces = myFaceDetector(img)
    persistent obj
    if isempty(obj)
        obj = cv.CascadeClassifier();
        obj.load(xmlFace);
    end

    if size(img,3) > 1
        gray = cv.cvtColor(img, 'RGB2GRAY');
    else
        gray = img;
    end
    gray = cv.equalizeHist(gray);
    faces = obj.detect(gray, 'ScaleFactor',1.4, 'MinNeighbors',2, ...
        'ScaleImage',true, 'MinSize',[30 30]);
end

##### SOURCE END #####
-->
   </body>
</html>