<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>AKAZE local features matching</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2017-11-27">
      <meta name="DC.source" content="akaze_match_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">AKAZE local features matching</h1>
         <!--introduction-->
         <p>In this demo, we will learn how to use AKAZE local features to detect and match keypoints on two images. We will find keypoints
            on a pair of images with given homography matrix, match them and count the number of inliers (i.e. matches that fit in the
            given homography).
         </p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://docs.opencv.org/3.2.0/db/d70/tutorial_akaze_matching.html">https://docs.opencv.org/3.2.0/db/d70/tutorial_akaze_matching.html</a></li>
               <li><a href="https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/features2D/AKAZE_match.cpp">https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/features2D/AKAZE_match.cpp</a></li>
               <li><a href="https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/xfeatures2D/LATCH_match.cpp">https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/xfeatures2D/LATCH_match.cpp</a></li>
            </ul>
         </div>
         <!--/introduction-->
         <h2 id="toc">Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Description</a></li>
               <li><a href="#3">Code</a></li>
               <li><a href="#13">References</a></li>
            </ul>
         </div>
         <h2 id="2">Description</h2>
         <p>You can find expanded version of this example <a href="https://github.com/pablofdezalc/test_kaze_akaze_opencv">here</a>.
         </p>
         <p>We are going to use images 1 and 3 from <i>Graffity</i> sequence of Oxford dataset.
         </p>
         <p><img src="https://docs.opencv.org/3.2.0/graf.png"></p>
         <p>Homography is given by a 3-by-3 matrix shown below.</p>
         <h2 id="3">Code</h2>
         <p>Options</p><pre class="codeinput">inlier_threshold = 2.5;  <span class="comment">% Distance threshold to identify inliers</span>
nn_match_ratio = 0.8;    <span class="comment">% Nearest neighbor</span></pre><p>Load grayscale images</p><pre class="codeinput">fnames = {
    fullfile(mexopencv.root(), <span class="string">'test'</span>, <span class="string">'graf1.png'</span>)
    fullfile(mexopencv.root(), <span class="string">'test'</span>, <span class="string">'graf3.png'</span>)
};
imgs = cell(size(fnames));
<span class="keyword">for</span> i=1:numel(fnames)
    <span class="comment">% downloading if necessary</span>
    <span class="keyword">if</span> exist(fnames{i}, <span class="string">'file'</span>) ~= 2
        disp(<span class="string">'Downloading image...'</span>)
        [~,name,ext] = fileparts(fnames{i});
        baseURL = <span class="string">'https://cdn.rawgit.com/opencv/opencv/3.2.0/samples/data/'</span>;
        urlwrite([baseURL, name, ext], fnames{i});
    <span class="keyword">end</span>
    <span class="comment">% read image</span>
    imgs{i} = cv.imread(fnames{i}, <span class="string">'Grayscale'</span>,true);
<span class="keyword">end</span>
[img1, img2] = deal(imgs{:});
whos <span class="string">img1</span> <span class="string">img2</span></pre><pre class="codeoutput">  Name        Size              Bytes  Class    Attributes

  img1      640x800            512000  uint8              
  img2      640x800            512000  uint8              

</pre><p>ground-truth homography</p><pre class="codeinput">H = [
    7.6285898e-01  -2.9922929e-01   2.2567123e+02
    3.3443473e-01   1.0143901e+00  -7.6999973e+01
    3.4663091e-04  -1.4364524e-05   1.0000000e+00
];
display(H)</pre><pre class="codeoutput">H =
    0.7629   -0.2992  225.6712
    0.3344    1.0144  -77.0000
    0.0003   -0.0000    1.0000
</pre><p>Detect keypoints and compute descriptors using AKAZE</p><pre class="codeinput"><span class="keyword">if</span> true
    name = <span class="string">'AKAZE'</span>;
    akaze = cv.AKAZE();
    [kpts1, desc1] = akaze.detectAndCompute(img1);
    [kpts2, desc2] = akaze.detectAndCompute(img2);
<span class="keyword">else</span>
    <span class="comment">% requires xfeatures2d from opencv_contrib</span>
    name = <span class="string">'LATCH'</span>;
    orb = cv.ORB(<span class="string">'MaxFeatures'</span>,10000);
    kpts1 = orb.detect(img1);
    kpts2 = orb.detect(img2);
    latch = cv.LATCH();
    desc1 = latch.compute(img1, kpts1);
    desc2 = latch.compute(img2, kpts2);
<span class="keyword">end</span>
whos <span class="string">kpts1</span> <span class="string">kpts2</span> <span class="string">desc1</span> <span class="string">desc2</span></pre><pre class="codeoutput">  Name          Size                Bytes  Class     Attributes

  desc1      2418x61               147498  uint8               
  desc2      2884x61               175924  uint8               
  kpts1         1x2418            1760688  struct              
  kpts2         1x2884            2099936  struct              

</pre><p>Use brute-force matcher to find 2-nn matches. We use Hamming distance, because AKAZE uses binary descriptor by default.</p><pre class="codeinput">matcher = cv.DescriptorMatcher(<span class="string">'BruteForce-Hamming'</span>);
matches = matcher.knnMatch(desc1, desc2, 2);
whos <span class="string">matches</span></pre><pre class="codeoutput">  Name         Size                Bytes  Class    Attributes

  matches      1x2418            3211104  cell               

</pre><p>Use 2-nn matches to find correct keypoint matches. If the closest match is <tt>ratio</tt> closer than the second closest one, then the match is correct.
         </p><pre class="codeinput">idx = cellfun(@(m) m(1).distance &lt; nn_match_ratio * m(2).distance, matches);
matches = cellfun(@(m) m(1), matches(idx));
kp1 = kpts1([matches.queryIdx] + 1);
kp2 = kpts2([matches.trainIdx] + 1);
whos <span class="string">matches</span> <span class="string">kp1</span> <span class="string">kp2</span></pre><pre class="codeoutput">  Name         Size              Bytes  Class     Attributes

  kp1          1x382            278480  struct              
  kp2          1x382            278480  struct              
  matches      1x382            183616  struct              

</pre><p>Check if our matches fit in the homography model. If the distance from first keypoint's projection to the second keypoint
            is less than threshold, then it it fits in the homography.
         </p><pre class="codeinput">pts1 = cat(1, kp1.pt);
pts1(:,3) = 1;
pts1 = (H * pts1.').';
pts1 = bsxfun(@rdivide, pts1(:,1:2), pts1(:,3));
pts2 = cat(1, kp2.pt);
d = sqrt(sum((pts1 - pts2).^2, 2));
idx = d &lt; inlier_threshold;
whos <span class="string">pts1</span> <span class="string">pts2</span>
fprintf(<span class="string">'%d inliers\n'</span>, nnz(idx));</pre><pre class="codeoutput">  Name        Size            Bytes  Class     Attributes

  pts1      382x2              6112  double              
  pts2      382x2              6112  double              

267 inliers
</pre><p>We create a new set of matches for the inliers, because it is required by the drawing function.</p><pre class="codeinput">kp1_good = kp1(idx);
kp2_good = kp2(idx);
matches_good = matches(idx);
<span class="keyword">for</span> i=1:numel(matches_good)
    matches_good(i).queryIdx = i-1;
    matches_good(i).trainIdx = i-1;
<span class="keyword">end</span>
whos <span class="string">kp1_good</span> <span class="string">kp2_good</span> <span class="string">matches_good</span></pre><pre class="codeoutput">  Name              Size              Bytes  Class     Attributes

  kp1_good          1x267            194760  struct              
  kp2_good          1x267            194760  struct              
  matches_good      1x267            128416  struct              

</pre><p>Show the result</p><pre class="codeinput">res = cv.drawMatches(img1, kp1_good, img2, kp2_good, matches_good);
imshow(res), title(name)</pre><pre class="codeoutput">Warning: Image is too big to fit on screen; displaying at 67% 
</pre><img src="akaze_match_demo_01.png"><p>Print some statistics</p><pre class="codeinput">fprintf(<span class="string">'%s Matching Results:\n'</span>, name);
fprintf(<span class="string">'Keypoints 1: %d\n'</span>, numel(kpts1));
fprintf(<span class="string">'Keypoints 2: %d\n'</span>, numel(kpts2));
fprintf(<span class="string">'Matches: %d\n'</span>, numel(matches));
fprintf(<span class="string">'Inliers: %d\n'</span>, numel(matches_good));
fprintf(<span class="string">'Inliers Ratio: %f\n'</span>, numel(matches_good) / numel(matches));</pre><pre class="codeoutput">AKAZE Matching Results:
Keypoints 1: 2418
Keypoints 2: 2884
Matches: 382
Inliers: 267
Inliers Ratio: 0.698953
</pre><h2 id="13">References</h2>
         <div>
            <ul>
               <li><b>(AKAZE)</b> Pablo F Alcantarilla, Jesus Nuevo, and Adrien Bartoli. "Fast   explicit diffusion for accelerated features in nonlinear scale
                  spaces".   Trans. Pattern Anal. Machine Intell, 34(7):1281-1298, 2011.
               </li>
               <li><b>(LATCH)</b> Gil Levi and Tal Hassner, "LATCH: Learned Arrangements of   Three Patch Codes", arXiv preprint arXiv:1501.03719, 15 Jan.
                  2015
               </li>
            </ul>
         </div>
         <div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div>
      <!--
##### SOURCE BEGIN #####
%% AKAZE local features matching
%
% In this demo, we will learn how to use AKAZE local features to detect and
% match keypoints on two images. We will find keypoints on a pair of images
% with given homography matrix, match them and count the number of inliers
% (i.e. matches that fit in the given homography).
%
% Sources:
%
% * <https://docs.opencv.org/3.2.0/db/d70/tutorial_akaze_matching.html>
% * <https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/features2D/AKAZE_match.cpp>
% * <https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/xfeatures2D/LATCH_match.cpp>
%

%% Description
%
% You can find expanded version of this example
% <https://github.com/pablofdezalc/test_kaze_akaze_opencv here>.
%
% We are going to use images 1 and 3 from _Graffity_ sequence of Oxford
% dataset.
%
% <<https://docs.opencv.org/3.2.0/graf.png>>
%
% Homography is given by a 3-by-3 matrix shown below.
%

%% Code
% Options
inlier_threshold = 2.5;  % Distance threshold to identify inliers
nn_match_ratio = 0.8;    % Nearest neighbor

%%
% Load grayscale images
fnames = {
    fullfile(mexopencv.root(), 'test', 'graf1.png')
    fullfile(mexopencv.root(), 'test', 'graf3.png')
};
imgs = cell(size(fnames));
for i=1:numel(fnames)
    % downloading if necessary
    if exist(fnames{i}, 'file') ~= 2
        disp('Downloading image...')
        [~,name,ext] = fileparts(fnames{i});
        baseURL = 'https://cdn.rawgit.com/opencv/opencv/3.2.0/samples/data/';
        urlwrite([baseURL, name, ext], fnames{i});
    end
    % read image
    imgs{i} = cv.imread(fnames{i}, 'Grayscale',true);
end
[img1, img2] = deal(imgs{:});
whos img1 img2

%%
% ground-truth homography
H = [
    7.6285898e-01  -2.9922929e-01   2.2567123e+02
    3.3443473e-01   1.0143901e+00  -7.6999973e+01
    3.4663091e-04  -1.4364524e-05   1.0000000e+00
];
display(H)

%%
% Detect keypoints and compute descriptors using AKAZE
if true
    name = 'AKAZE';
    akaze = cv.AKAZE();
    [kpts1, desc1] = akaze.detectAndCompute(img1);
    [kpts2, desc2] = akaze.detectAndCompute(img2);
else
    % requires xfeatures2d from opencv_contrib
    name = 'LATCH';
    orb = cv.ORB('MaxFeatures',10000);
    kpts1 = orb.detect(img1);
    kpts2 = orb.detect(img2);
    latch = cv.LATCH();
    desc1 = latch.compute(img1, kpts1);
    desc2 = latch.compute(img2, kpts2);
end
whos kpts1 kpts2 desc1 desc2

%%
% Use brute-force matcher to find 2-nn matches.
% We use Hamming distance, because AKAZE uses binary descriptor by default.
matcher = cv.DescriptorMatcher('BruteForce-Hamming');
matches = matcher.knnMatch(desc1, desc2, 2);
whos matches

%%
% Use 2-nn matches to find correct keypoint matches.
% If the closest match is |ratio| closer than the second closest one, then the
% match is correct.
idx = cellfun(@(m) m(1).distance < nn_match_ratio * m(2).distance, matches);
matches = cellfun(@(m) m(1), matches(idx));
kp1 = kpts1([matches.queryIdx] + 1);
kp2 = kpts2([matches.trainIdx] + 1);
whos matches kp1 kp2

%%
% Check if our matches fit in the homography model.
% If the distance from first keypoint's projection to the second keypoint
% is less than threshold, then it it fits in the homography.
pts1 = cat(1, kp1.pt);
pts1(:,3) = 1;
pts1 = (H * pts1.').';
pts1 = bsxfun(@rdivide, pts1(:,1:2), pts1(:,3));
pts2 = cat(1, kp2.pt);
d = sqrt(sum((pts1 - pts2).^2, 2));
idx = d < inlier_threshold;
whos pts1 pts2
fprintf('%d inliers\n', nnz(idx));

%%
% We create a new set of matches for the inliers, because it is required
% by the drawing function.
kp1_good = kp1(idx);
kp2_good = kp2(idx);
matches_good = matches(idx);
for i=1:numel(matches_good)
    matches_good(i).queryIdx = i-1;
    matches_good(i).trainIdx = i-1;
end
whos kp1_good kp2_good matches_good

%%
% Show the result
res = cv.drawMatches(img1, kp1_good, img2, kp2_good, matches_good);
imshow(res), title(name)

%%
% Print some statistics
fprintf('%s Matching Results:\n', name);
fprintf('Keypoints 1: %d\n', numel(kpts1));
fprintf('Keypoints 2: %d\n', numel(kpts2));
fprintf('Matches: %d\n', numel(matches));
fprintf('Inliers: %d\n', numel(matches_good));
fprintf('Inliers Ratio: %f\n', numel(matches_good) / numel(matches));

%% References
%
% * *(AKAZE)* Pablo F Alcantarilla, Jesus Nuevo, and Adrien Bartoli. "Fast
%   explicit diffusion for accelerated features in nonlinear scale spaces".
%   Trans. Pattern Anal. Machine Intell, 34(7):1281-1298, 2011.
% * *(LATCH)* Gil Levi and Tal Hassner, "LATCH: Learned Arrangements of
%   Three Patch Codes", arXiv preprint arXiv:1501.03719, 15 Jan. 2015
%

##### SOURCE END #####
-->
   </body>
</html>