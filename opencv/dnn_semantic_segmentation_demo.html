<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>DNN Semantic Segmentation</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2018-02-20">
      <meta name="DC.source" content="dnn_semantic_segmentation_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">DNN Semantic Segmentation</h1>
         <p>This sample demonstrates semantic segmentation, where we label each pixel in the image with a category label.</p>
         <div>
            <ul>
               <li><a href="https://arxiv.org/abs/1605.06211">FCN</a></li>
               <li><a href="https://arxiv.org/abs/1606.02147">ENet</a></li>
            </ul>
         </div>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/fcn_semsegm.cpp">https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/fcn_semsegm.cpp</a></li>
               <li><a href="https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/torch_enet.cpp">https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/torch_enet.cpp</a></li>
            </ul>
         </div><pre class="codeinput"><span class="keyword">function</span> dnn_semantic_segmentation_demo(im, name, crop)
    <span class="comment">% input image (BGR channel order)</span>
    <span class="keyword">if</span> nargin &lt; 1 || isempty(im)
        im = fullfile(mexopencv.root(), <span class="string">'test'</span>, <span class="string">'rgb.jpg'</span>);
    <span class="keyword">end</span>
    img = cv.imread(im, <span class="string">'Color'</span>,true, <span class="string">'FlipChannels'</span>,false);

    <span class="comment">% import pretrained model</span>
    <span class="keyword">if</span> nargin &lt; 2, name = <span class="string">'FCN'</span>; <span class="keyword">end</span>
    fprintf(<span class="string">'Load model...   '</span>); tic;
    <span class="keyword">switch</span> lower(name)
        <span class="keyword">case</span> <span class="string">'fcn'</span>
            <span class="comment">% PASCAL VOC</span>
            [net, labels, blobOpts] = FCN();
        <span class="keyword">case</span> <span class="string">'enet'</span>
            <span class="comment">% Cityscapes</span>
            [net, labels, blobOpts] = ENet();
        <span class="keyword">otherwise</span>
            error(<span class="string">'Unrecognized model %s'</span>, name)
    <span class="keyword">end</span>
    toc;
    assert(~net.empty(), <span class="string">'Failed to read network %s'</span>, name);

    <span class="comment">% feed image to network</span>
    <span class="keyword">if</span> nargin &lt; 3, crop = true; <span class="keyword">end</span>
    blobOpts = [<span class="string">'Crop'</span>,crop, blobOpts];
    opts = parseBlobOpts(blobOpts{:});
    blob = cv.Net.blobFromImages(img, blobOpts{:});
    net.setInput(blob);

    <span class="comment">% run forward pass</span>
    fprintf(<span class="string">'Forward pass... '</span>); tic;
    score = net.forward();  <span class="comment">% 1-by-nclasses-by-nrows-by-ncols</span>
    toc;

    <span class="comment">% prepare output image</span>
    out = outputImage(img, blob, opts);

    <span class="comment">% pixel-wise segmentation (predict class with max score)</span>
    score = permute(score, [3 4 2 1]);  <span class="comment">% H-by-W-by-nclasses</span>
    [S,L] = max(score, [], 3);

    <span class="comment">% show count of pixels per class</span>
    <span class="keyword">if</span> ~mexopencv.isOctave() &amp;&amp; mexopencv.require(<span class="string">'stats'</span>)
        disp(<span class="string">'Pixel Segmentation Summary:'</span>)
        tabulate({labels(L(:)).name})
    <span class="keyword">end</span>

    <span class="comment">% show segmentation with color-coded classes</span>
    rgb = reshape(cat(1, labels(L(:)).color), [size(L) 3]);  <span class="comment">% label2rgb</span>
    out = cv.addWeighted(out, 0.3, rgb, 0.7, 0.0);
    imshow(out), title(<span class="string">'Semantic Segmentation'</span>)
    <span class="keyword">if</span> ~mexopencv.isOctave()
        <span class="comment">% show label/score of current pixel in tooltips</span>
        disp(<span class="string">'Move data cursor over pixels to see segmentation labels'</span>)
        hDCM = datacursormode(gcf);
        set(hDCM, <span class="string">'Enable'</span>,<span class="string">'on'</span>, <span class="string">'SnapToDataVertex'</span>,<span class="string">'on'</span>, <span class="keyword">...</span>
            <span class="string">'UpdateFcn'</span>,@(~,e) {
                sprintf(<span class="string">'Label: %s'</span>, labels(L(e.Position(2), e.Position(1))).name)
                sprintf(<span class="string">'Score: %g'</span>, S(e.Position(2), e.Position(1)))
            });
    <span class="keyword">end</span>

    <span class="comment">% show legend of color-coded classes</span>
    lgnd = createLabelsLegend(labels);
    figure, imshow(lgnd), title(<span class="string">'Class Labels'</span>)
<span class="keyword">end</span>

<span class="comment">% --- Helper functions ---</span>

<span class="keyword">function</span> dname = get_dnn_dir(dname)
    <span class="comment">%GET_DNN_DIR  Path to model files, and show where to get them if missing</span>

    dname = fullfile(mexopencv.root(), <span class="string">'test'</span>, <span class="string">'dnn'</span>, dname);
    b = isdir(dname);
    <span class="keyword">if</span> ~b
        <span class="comment">% display help of calling function</span>
        <span class="comment">% (assumed to be a local function in current file)</span>
        st = dbstack(1);
        help([mfilename() filemarker() st(1).name])
    <span class="keyword">end</span>
    assert(b, <span class="string">'Missing model: %s'</span>, dname);
<span class="keyword">end</span>

<span class="keyword">function</span> labels = readLabelsColors(labelsFile, addBG)
    <span class="keyword">if</span> nargin &lt; 2, addBG = false; <span class="keyword">end</span>
    fid = fopen(labelsFile, <span class="string">'rt'</span>);
    C = textscan(fid, <span class="string">'%s %d %d %d'</span>, <span class="string">'CollectOutput'</span>,true);
    fclose(fid);
    name = C{1};
    color = uint8(C{2});
    <span class="keyword">if</span> addBG
        name = [<span class="string">'background'</span>; name];
        color = [0 0 0; color];
    <span class="keyword">end</span>
    id = 0:(numel(name) - 1);  <span class="comment">% first label 0 corresponds to background</span>
    labels = struct(<span class="string">'id'</span>,num2cell(id(:),2), <span class="string">'name'</span>,name, <span class="string">'color'</span>,num2cell(color,2));
<span class="keyword">end</span>

<span class="keyword">function</span> opts = parseBlobOpts(varargin)
    p = inputParser();
    p.addParameter(<span class="string">'ScaleFactor'</span>, 1.0);
    p.addParameter(<span class="string">'Size'</span>, [0 0]);   <span class="comment">% [w,h]</span>
    p.addParameter(<span class="string">'Mean'</span>, [0 0 0]); <span class="comment">% [r,g,b]</span>
    p.addParameter(<span class="string">'SwapRB'</span>, true);
    p.addParameter(<span class="string">'Crop'</span>, true);
    p.parse(varargin{:});
    opts = p.Results;
<span class="keyword">end</span>

<span class="keyword">function</span> img = imageFromBlob(blob, opts)
    img = permute(blob, [3 4 2 1]); <span class="comment">% NCHW -&gt; HWCN</span>
    img = img / opts.ScaleFactor;
    <span class="keyword">if</span> false &amp;&amp; opts.SwapRB
        opts.Mean([1 3]) = opts.Mean([3 1]);
    <span class="keyword">end</span>
    img = bsxfun(@plus, img, reshape(opts.Mean, 1, 1, []));
    img = uint8(round(img));
<span class="keyword">end</span>

<span class="keyword">function</span> img = cropImage(img, opts)
    <span class="comment">% https://github.com/opencv/opencv/blob/3.3.1/modules/dnn/src/dnn.cpp#L95-L176</span>
    imgSz = [size(img,2) size(img,1)];
    <span class="keyword">if</span> ~isequal(imgSz, opts.Size)
        <span class="keyword">if</span> opts.Crop
            <span class="comment">% resize (preserving aspect-ratio) with center-cropping</span>
            sf = max(opts.Size ./ imgSz);
            img = cv.resize(img, sf, sf);
            imgSz = [size(img,2) size(img,1)];
            r = [fix((imgSz - opts.Size)/2) opts.Size];
            img = cv.Rect.crop(img, r);
        <span class="keyword">else</span>
            <span class="comment">% direct resize (stretched) without cropping</span>
            img = cv.resize(img, opts.Size);
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> out = outputImage(img, blob, opts)
    <span class="keyword">if</span> opts.Crop
        <span class="comment">% center cropped as fed to network</span>
        out = cropImage(img, opts);
    <span class="keyword">else</span>
        <span class="comment">% resized image (squashed) as fed to network</span>
        out = imageFromBlob(blob, opts);
    <span class="keyword">end</span>
    out = flip(out, 3);  <span class="comment">% BGR to RGB</span>
<span class="keyword">end</span>

<span class="keyword">function</span> img = createLabelsLegend(labels)
    img = cell(numel(labels),1);
    <span class="keyword">for</span> i=1:numel(labels)
        img{i} = repmat(reshape(labels(i).color, [1 1 3]), 20, 120, 1);
        img{i} = cv.putText(img{i}, labels(i).name, [0 15], <span class="keyword">...</span>
            <span class="string">'Color'</span>,[1 1 1]*255, <span class="string">'FontFace'</span>,<span class="string">'HersheySimplex'</span>, <span class="string">'FontScale'</span>,0.5);
    <span class="keyword">end</span>
    img = cat(1, img{:});
<span class="keyword">end</span>

<span class="comment">% --- Pretrained models ---</span>
<span class="comment">% See also: https://github.com/opencv/opencv_extra/blob/3.3.1/testdata/dnn/download_models.py</span>

<span class="keyword">function</span> [net, labels, blobOpts] = FCN()
    <span class="comment">%FCN  Fully Convolutional Networks, FCN-8s PASCAL VOC [Caffe]</span>
    <span class="comment">%</span>
    <span class="comment">% homepage = https://github.com/shelhamer/fcn.berkeleyvision.org</span>
    <span class="comment">%</span>
    <span class="comment">% ## Model</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/FCN/fcn8s-heavy-pascal.prototxt</span>
    <span class="comment">% url  = https://github.com/opencv/opencv/raw/3.3.1/samples/data/dnn/fcn8s-heavy-pascal.prototxt</span>
    <span class="comment">%</span>
    <span class="comment">% ## Weights</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/FCN/fcn8s-heavy-pascal.caffemodel</span>
    <span class="comment">% url  = http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel</span>
    <span class="comment">% hash = c449ea74dd7d83751d1357d6a8c323fcf4038962</span>
    <span class="comment">% size = 513 MB</span>
    <span class="comment">%</span>
    <span class="comment">% ## Classes</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/FCN/pascal-classes.txt</span>
    <span class="comment">% url  = https://github.com/opencv/opencv/raw/3.3.1/samples/data/dnn/pascal-classes.txt</span>
    <span class="comment">%</span>

    dname = get_dnn_dir(<span class="string">'FCN'</span>);
    net = cv.Net(<span class="string">'Caffe'</span>, <span class="keyword">...</span>
        fullfile(dname, <span class="string">'fcn8s-heavy-pascal.prototxt'</span>), <span class="keyword">...</span>
        fullfile(dname, <span class="string">'fcn8s-heavy-pascal.caffemodel'</span>));
    labels = readLabelsColors(fullfile(dname, <span class="string">'pascal-classes.txt'</span>), false);
    blobOpts = {<span class="string">'SwapRB'</span>,false, <span class="string">'Size'</span>,[500 500], <span class="string">'Mean'</span>,[104.00699, 116.66877, 122.67892]};
<span class="keyword">end</span>

<span class="keyword">function</span> [net, labels, blobOpts] = ENet()
    <span class="comment">%ENET  ENet on Cityscapes dataset [Torch]</span>
    <span class="comment">%</span>
    <span class="comment">% homepage = https://github.com/e-lab/ENet-training</span>
    <span class="comment">%</span>
    <span class="comment">% ## Model + Weights</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/ENet/model-cityscapes.net</span>
    <span class="comment">% url  = https://github.com/e-lab/ENet-training/releases/download/v1.cs/model-cityscapes.net</span>
    <span class="comment">% hash = b4123a73bf464b9ebe9cfc4ab9c2d5c72b161315</span>
    <span class="comment">% size = 3.08 MB</span>
    <span class="comment">%</span>
    <span class="comment">% ## Classes</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/ENet/enet-classes.txt</span>
    <span class="comment">% url  = https://github.com/opencv/opencv/raw/3.3.1/samples/data/dnn/enet-classes.txt</span>
    <span class="comment">%</span>

    dname = get_dnn_dir(<span class="string">'ENet'</span>);
    net = cv.Net(<span class="string">'Torch'</span>, <span class="keyword">...</span>
        fullfile(dname, <span class="string">'model-cityscapes.net'</span>));
    labels = readLabelsColors(fullfile(dname, <span class="string">'enet-classes.txt'</span>), false);
    blobOpts = {<span class="string">'SwapRB'</span>,true, <span class="string">'Size'</span>,[1024 512], <span class="string">'ScaleFactor'</span>,1/255};
<span class="keyword">end</span></pre><pre class="codeoutput">Load model...   Elapsed time is 1.278333 seconds.
Forward pass... Elapsed time is 5.553631 seconds.
Pixel Segmentation Summary:
       Value    Count   Percent
  background    172429     68.97%
      person    36320     14.53%
     bicycle    40702     16.28%
   motorbike      549      0.22%
Move data cursor over pixels to see segmentation labels
</pre><img src="dnn_semantic_segmentation_demo_01.png"><img src="dnn_semantic_segmentation_demo_02.png"><div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div>
      <!--
##### SOURCE BEGIN #####
%% DNN Semantic Segmentation
%
% This sample demonstrates semantic segmentation, where we label each pixel
% in the image with a category label.
%
% * <https://arxiv.org/abs/1605.06211 FCN>
% * <https://arxiv.org/abs/1606.02147 ENet>
%
% Sources:
%
% * <https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/fcn_semsegm.cpp>
% * <https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/torch_enet.cpp>
%

function dnn_semantic_segmentation_demo(im, name, crop)
    % input image (BGR channel order)
    if nargin < 1 || isempty(im)
        im = fullfile(mexopencv.root(), 'test', 'rgb.jpg');
    end
    img = cv.imread(im, 'Color',true, 'FlipChannels',false);

    % import pretrained model
    if nargin < 2, name = 'FCN'; end
    fprintf('Load model...   '); tic;
    switch lower(name)
        case 'fcn'
            % PASCAL VOC
            [net, labels, blobOpts] = FCN();
        case 'enet'
            % Cityscapes
            [net, labels, blobOpts] = ENet();
        otherwise
            error('Unrecognized model %s', name)
    end
    toc;
    assert(~net.empty(), 'Failed to read network %s', name);

    % feed image to network
    if nargin < 3, crop = true; end
    blobOpts = ['Crop',crop, blobOpts];
    opts = parseBlobOpts(blobOpts{:});
    blob = cv.Net.blobFromImages(img, blobOpts{:});
    net.setInput(blob);

    % run forward pass
    fprintf('Forward pass... '); tic;
    score = net.forward();  % 1-by-nclasses-by-nrows-by-ncols
    toc;

    % prepare output image
    out = outputImage(img, blob, opts);

    % pixel-wise segmentation (predict class with max score)
    score = permute(score, [3 4 2 1]);  % H-by-W-by-nclasses
    [S,L] = max(score, [], 3);

    % show count of pixels per class
    if ~mexopencv.isOctave() && mexopencv.require('stats')
        disp('Pixel Segmentation Summary:')
        tabulate({labels(L(:)).name})
    end

    % show segmentation with color-coded classes
    rgb = reshape(cat(1, labels(L(:)).color), [size(L) 3]);  % label2rgb
    out = cv.addWeighted(out, 0.3, rgb, 0.7, 0.0);
    imshow(out), title('Semantic Segmentation')
    if ~mexopencv.isOctave()
        % show label/score of current pixel in tooltips
        disp('Move data cursor over pixels to see segmentation labels')
        hDCM = datacursormode(gcf);
        set(hDCM, 'Enable','on', 'SnapToDataVertex','on', ...
            'UpdateFcn',@(~,e) {
                sprintf('Label: %s', labels(L(e.Position(2), e.Position(1))).name)
                sprintf('Score: %g', S(e.Position(2), e.Position(1)))
            });
    end

    % show legend of color-coded classes
    lgnd = createLabelsLegend(labels);
    figure, imshow(lgnd), title('Class Labels')
end

% REPLACE_WITH_DASH_DASH- Helper functions REPLACE_WITH_DASH_DASH-

function dname = get_dnn_dir(dname)
    %GET_DNN_DIR  Path to model files, and show where to get them if missing

    dname = fullfile(mexopencv.root(), 'test', 'dnn', dname);
    b = isdir(dname);
    if ~b
        % display help of calling function
        % (assumed to be a local function in current file)
        st = dbstack(1);
        help([mfilename() filemarker() st(1).name])
    end
    assert(b, 'Missing model: %s', dname);
end

function labels = readLabelsColors(labelsFile, addBG)
    if nargin < 2, addBG = false; end
    fid = fopen(labelsFile, 'rt');
    C = textscan(fid, '%s %d %d %d', 'CollectOutput',true);
    fclose(fid);
    name = C{1};
    color = uint8(C{2});
    if addBG
        name = ['background'; name];
        color = [0 0 0; color];
    end
    id = 0:(numel(name) - 1);  % first label 0 corresponds to background
    labels = struct('id',num2cell(id(:),2), 'name',name, 'color',num2cell(color,2));
end

function opts = parseBlobOpts(varargin)
    p = inputParser();
    p.addParameter('ScaleFactor', 1.0);
    p.addParameter('Size', [0 0]);   % [w,h]
    p.addParameter('Mean', [0 0 0]); % [r,g,b]
    p.addParameter('SwapRB', true);
    p.addParameter('Crop', true);
    p.parse(varargin{:});
    opts = p.Results;
end

function img = imageFromBlob(blob, opts)
    img = permute(blob, [3 4 2 1]); % NCHW -> HWCN
    img = img / opts.ScaleFactor;
    if false && opts.SwapRB
        opts.Mean([1 3]) = opts.Mean([3 1]);
    end
    img = bsxfun(@plus, img, reshape(opts.Mean, 1, 1, []));
    img = uint8(round(img));
end

function img = cropImage(img, opts)
    % https://github.com/opencv/opencv/blob/3.3.1/modules/dnn/src/dnn.cpp#L95-L176
    imgSz = [size(img,2) size(img,1)];
    if ~isequal(imgSz, opts.Size)
        if opts.Crop
            % resize (preserving aspect-ratio) with center-cropping
            sf = max(opts.Size ./ imgSz);
            img = cv.resize(img, sf, sf);
            imgSz = [size(img,2) size(img,1)];
            r = [fix((imgSz - opts.Size)/2) opts.Size];
            img = cv.Rect.crop(img, r);
        else
            % direct resize (stretched) without cropping
            img = cv.resize(img, opts.Size);
        end
    end
end

function out = outputImage(img, blob, opts)
    if opts.Crop
        % center cropped as fed to network
        out = cropImage(img, opts);
    else
        % resized image (squashed) as fed to network
        out = imageFromBlob(blob, opts);
    end
    out = flip(out, 3);  % BGR to RGB
end

function img = createLabelsLegend(labels)
    img = cell(numel(labels),1);
    for i=1:numel(labels)
        img{i} = repmat(reshape(labels(i).color, [1 1 3]), 20, 120, 1);
        img{i} = cv.putText(img{i}, labels(i).name, [0 15], ...
            'Color',[1 1 1]*255, 'FontFace','HersheySimplex', 'FontScale',0.5);
    end
    img = cat(1, img{:});
end

% REPLACE_WITH_DASH_DASH- Pretrained models REPLACE_WITH_DASH_DASH-
% See also: https://github.com/opencv/opencv_extra/blob/3.3.1/testdata/dnn/download_models.py

function [net, labels, blobOpts] = FCN()
    %FCN  Fully Convolutional Networks, FCN-8s PASCAL VOC [Caffe]
    %
    % homepage = https://github.com/shelhamer/fcn.berkeleyvision.org
    %
    % ## Model
    %
    % file = test/dnn/FCN/fcn8s-heavy-pascal.prototxt
    % url  = https://github.com/opencv/opencv/raw/3.3.1/samples/data/dnn/fcn8s-heavy-pascal.prototxt
    %
    % ## Weights
    %
    % file = test/dnn/FCN/fcn8s-heavy-pascal.caffemodel
    % url  = http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel
    % hash = c449ea74dd7d83751d1357d6a8c323fcf4038962
    % size = 513 MB
    %
    % ## Classes
    %
    % file = test/dnn/FCN/pascal-classes.txt
    % url  = https://github.com/opencv/opencv/raw/3.3.1/samples/data/dnn/pascal-classes.txt
    %

    dname = get_dnn_dir('FCN');
    net = cv.Net('Caffe', ...
        fullfile(dname, 'fcn8s-heavy-pascal.prototxt'), ...
        fullfile(dname, 'fcn8s-heavy-pascal.caffemodel'));
    labels = readLabelsColors(fullfile(dname, 'pascal-classes.txt'), false);
    blobOpts = {'SwapRB',false, 'Size',[500 500], 'Mean',[104.00699, 116.66877, 122.67892]};
end

function [net, labels, blobOpts] = ENet()
    %ENET  ENet on Cityscapes dataset [Torch]
    %
    % homepage = https://github.com/e-lab/ENet-training
    %
    % ## Model + Weights
    %
    % file = test/dnn/ENet/model-cityscapes.net
    % url  = https://github.com/e-lab/ENet-training/releases/download/v1.cs/model-cityscapes.net
    % hash = b4123a73bf464b9ebe9cfc4ab9c2d5c72b161315
    % size = 3.08 MB
    %
    % ## Classes
    %
    % file = test/dnn/ENet/enet-classes.txt
    % url  = https://github.com/opencv/opencv/raw/3.3.1/samples/data/dnn/enet-classes.txt
    %

    dname = get_dnn_dir('ENet');
    net = cv.Net('Torch', ...
        fullfile(dname, 'model-cityscapes.net'));
    labels = readLabelsColors(fullfile(dname, 'enet-classes.txt'), false);
    blobOpts = {'SwapRB',true, 'Size',[1024 512], 'ScaleFactor',1/255};
end

##### SOURCE END #####
-->
   </body>
</html>