<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>Affine invariant feature-based image matching</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2017-11-27">
      <meta name="DC.source" content="asift_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">Affine invariant feature-based image matching</h1>
         <p>This sample is similar to <tt>feature_homography_demo.m</tt>, but uses the affine transformation space sampling technique, called <a href="http://www.ipol.im/pub/algo/my_affine_sift/">ASIFT</a>. While the original implementation is based on SIFT, you can try to use SURF or ORB detectors instead. Homography RANSAC
            is used to reject outliers.
         </p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://github.com/opencv/opencv/blob/3.2.0/samples/python/asift.py">https://github.com/opencv/opencv/blob/3.2.0/samples/python/asift.py</a></li>
            </ul>
         </div><pre class="codeinput"><span class="keyword">function</span> asift_demo()
    <span class="comment">% load input grayscale images</span>
    files = {
        fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'aero1.jpg'</span>)
        fullfile(mexopencv.root(),<span class="string">'test'</span>,<span class="string">'aero3.jpg'</span>)
    };
    <span class="keyword">for</span> i=1:numel(files)
        <span class="keyword">if</span> exist(files{i}, <span class="string">'file'</span>) ~= 2
            disp(<span class="string">'Downloading...'</span>)
            url = <span class="string">'https://cdn.rawgit.com/opencv/opencv/3.2.0/samples/data/'</span>;
            [~,fname,ext] = fileparts(files{i});
            urlwrite([url fname ext], files{i});
        <span class="keyword">end</span>
    <span class="keyword">end</span>
    img1 = cv.imread(files{1}, <span class="string">'Grayscale'</span>,true);
    img2 = cv.imread(files{2}, <span class="string">'Grayscale'</span>,true);

    <span class="comment">% ASIFT</span>
    [vis, H] = run_asift(img1, img2, <span class="string">'BRISK'</span>, true);
    imshow(vis), title(<span class="string">'ASIFT'</span>)

    <span class="comment">% interactive exploration</span>
    <span class="keyword">if</span> ~mexopencv.isOctave() &amp;&amp; mexopencv.require(<span class="string">'images'</span>)
        <span class="comment">% interactively select region in img1</span>
        pos1 = select_poly(gca) - 1;
        <span class="keyword">if</span> isempty(pos1), <span class="keyword">return</span>; <span class="keyword">end</span>


        <span class="comment">% apply homography</span>
        pos2 = cv.perspectiveTransform(pos1, H);
        pos2(:,1) = pos2(:,1) + size(img1,2);

        <span class="comment">% draw region in img1 and transformed region in img2</span>
        vis = cv.polylines(vis, pos1, <span class="string">'Closed'</span>,true, <span class="keyword">...</span>
            <span class="string">'Color'</span>,[0 255 0], <span class="string">'Thickness'</span>,2, <span class="string">'LineType'</span>,<span class="string">'AA'</span>);
        vis = cv.polylines(vis, pos2, <span class="string">'Closed'</span>,true, <span class="keyword">...</span>
            <span class="string">'Color'</span>,[0 255 0], <span class="string">'Thickness'</span>,2, <span class="string">'LineType'</span>,<span class="string">'AA'</span>);
        imshow(vis), title(<span class="string">'ASIFT'</span>)
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> [vis, H] = run_asift(img1, img2, feature_name, use_flann)
    <span class="comment">%RUN_ASIFT  Run ASIFT algorithm, estimate homography and visualize matches</span>

    <span class="comment">% create detector and matcher objects</span>
    [detector, matcher] = init_feature(feature_name, use_flann);

    <span class="comment">% detect features using ASIFT method</span>
    disp(<span class="string">'Detecting...'</span>)
    tic, [kp1, desc1] = affine_detect(detector, img1); toc
    tic, [kp2, desc2] = affine_detect(detector, img2); toc
    fprintf(<span class="string">'img1: %d features, img2: %d features\n'</span>, numel(kp1), numel(kp2));

    <span class="comment">% match ASIFT features</span>
    disp(<span class="string">'Matching...'</span>)
    tic, matches = matcher.knnMatch(desc1, desc2, 2); toc
    fprintf(<span class="string">'%d matches\n'</span>, numel(matches));

    <span class="comment">% filter matches</span>
    [matches, p1, p2] = filter_matches(kp1, kp2, matches);
    fprintf(<span class="string">'%d good matches\n'</span>, numel(matches));
    assert(numel(matches) &gt;= 4, <span class="string">'not enough matches for homography estimation'</span>);

    <span class="comment">% estimate homography with RANSAC method</span>
    [H,inliers] = cv.findHomography(p1, p2, <span class="string">'Method'</span>,<span class="string">'Ransac'</span>);
    assert(~isempty(H), <span class="string">'homography estimation failed'</span>);
    inliers = logical(inliers);
    fprintf(<span class="string">'%d inliers\n'</span>, nnz(inliers));

    <span class="comment">% visualize good and inlier matches</span>
    vis = cv.drawMatches(img1, kp1, img2, kp2, matches, <span class="keyword">...</span>
        <span class="string">'NotDrawSinglePoints'</span>,true, <span class="string">'MatchesMask'</span>,inliers);
<span class="keyword">end</span>

<span class="keyword">function</span> [detector, matcher] = init_feature(feature_name, use_flann)
    <span class="comment">%INIT_FEATURE  Create detector and matcher objects</span>

    <span class="comment">% detector</span>
    <span class="keyword">switch</span> upper(feature_name)
        <span class="keyword">case</span> <span class="string">'SURF'</span>
            detector = cv.SURF(<span class="string">'HessianThreshold'</span>,400);
        <span class="keyword">case</span> <span class="string">'SIFT'</span>
            detector = cv.SIFT();
        <span class="keyword">case</span> <span class="string">'ORB'</span>
            detector = cv.ORB();
        <span class="keyword">case</span> <span class="string">'BRISK'</span>
            detector = cv.BRISK();
        <span class="keyword">case</span> <span class="string">'AKAZE'</span>
            detector = cv.AKAZE();
        <span class="keyword">case</span> <span class="string">'KAZE'</span>
            detector = cv.KAZE();
        <span class="keyword">otherwise</span>
            error(<span class="string">'unrecognized feature: %s'</span>, feature_name)
    <span class="keyword">end</span>

    <span class="comment">% matcher</span>
    <span class="keyword">if</span> use_flann
        <span class="keyword">if</span> ~isempty(strfind(detector.defaultNorm(), <span class="string">'Hamming'</span>))
            opts = {<span class="string">'LSH'</span>, <span class="string">'TableNumber'</span>,6, <span class="string">'KeySize'</span>,12, <span class="string">'MultiProbeLevel'</span>,1};
        <span class="keyword">else</span>
            opts = {<span class="string">'KDTree'</span>, <span class="string">'Trees'</span>,5};
        <span class="keyword">end</span>
        matcher = cv.DescriptorMatcher(<span class="string">'FlannBasedMatcher'</span>, <span class="string">'Index'</span>,opts);
    <span class="keyword">else</span>
        matcher = cv.DescriptorMatcher(<span class="string">'BFMatcher'</span>, <span class="keyword">...</span>
            <span class="string">'NormType'</span>,detector.defaultNorm());
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> [kpts, descs] = affine_detect(detector, img, mask)
    <span class="comment">%AFFINE_DETECT  Affine-SIFT (ASIFT) algorithm</span>
    <span class="comment">%</span>
    <span class="comment">%     [kpts, descs] = affine_detect(detector, img)</span>
    <span class="comment">%     [kpts, descs] = affine_detect(detector, img, mask)</span>
    <span class="comment">%</span>
    <span class="comment">% ## Input</span>
    <span class="comment">% * __detector__ detector object</span>
    <span class="comment">% * __img__ input image</span>
    <span class="comment">% * __mask__ optional mask, default all image included</span>
    <span class="comment">%</span>
    <span class="comment">% ## Output</span>
    <span class="comment">% * __kpts__ concatenated keypoints</span>
    <span class="comment">% * __descs__ corresponding concatenated descriptors</span>
    <span class="comment">%</span>
    <span class="comment">% Applies a set of affine transormations to the image, detect keypoints,</span>
    <span class="comment">% and reproject them into initial image coordinates.</span>
    <span class="comment">% See: http://www.ipol.im/pub/algo/my_affine_sift/ for the details.</span>
    <span class="comment">%</span>

    <span class="comment">% default mask if not specified</span>
    <span class="keyword">if</span> nargin &lt; 3
        mask = 255 * ones(size(img,1), size(img,2), <span class="string">'uint8'</span>);
    <span class="keyword">end</span>

    <span class="comment">% parameter sampling: Tilt and Phi</span>
    <span class="comment">% (simulates all possible affine distortions caused by the change of</span>
    <span class="comment">% camera optical axis orientation from a frontal position)</span>
    params = [1 0];  <span class="comment">% no tilt and no rotation, thus original image</span>
    <span class="keyword">for</span> t = sqrt(2).^(1:5)       <span class="comment">% geometric series 1, a, a^2, .., a^n; a=sqrt(2), n=5</span>
        <span class="keyword">for</span> p = 0:72/t:180-72/t  <span class="comment">% arithmetic series 0, b/t, .., k*b/t &lt; 180; b=72</span>
            params(end+1,:) = [t, p];
        <span class="keyword">end</span>
    <span class="keyword">end</span>

    <span class="comment">% for each pair of distortion parameters</span>
    <span class="comment">% (this loop is candidate for parallelization, i.e parfor)</span>
    N = size(params,1);
    kpts = cell(N, 1);
    descs = cell(N, 1);
    <span class="keyword">for</span> i=1:N
        <span class="comment">% transform image</span>
        [timg, tmask, Ai] = affine_skew(params(i,1), params(i,2), img, mask);

        <span class="comment">% detect features using a similarity invariant matching method (SIFT)</span>
        [kp, feat] = detector.detectAndCompute(timg, <span class="string">'Mask'</span>,tmask);

        <span class="comment">%TODO: Remove keypoints close to the boundary of the transformed image</span>

        <span class="comment">% project keypoints from the coordinates of the rotated and tilted</span>
        <span class="comment">% image back to the original image corrdinates</span>
        <span class="keyword">for</span> j=1:numel(kp)
            kp(j).pt = [kp(j).pt, 1] * Ai.';
        <span class="keyword">end</span>
        kpts{i} = kp(:);
        descs{i} = feat;
    <span class="keyword">end</span>

    <span class="comment">% concatenate all features from all simulated images</span>
    kpts = cat(1, kpts{:});
    descs = cat(1, descs{:});
<span class="keyword">end</span>

<span class="keyword">function</span> [img, mask, Ai] = affine_skew(tilt, phi, img, mask)
    <span class="comment">%AFFINE_SKEW  Transform image/mask by an affine distortion</span>
    <span class="comment">%</span>
    <span class="comment">%     [img, mask, Ai] = affine_skew(tilt, phi, img, mask)</span>
    <span class="comment">%</span>
    <span class="comment">% ## Input</span>
    <span class="comment">% * __tilt__ tilt</span>
    <span class="comment">% * __phi__ rotation angle in degrees</span>
    <span class="comment">% * __img__ image</span>
    <span class="comment">% * __mask__ mask</span>
    <span class="comment">%</span>
    <span class="comment">% ## Output</span>
    <span class="comment">% * __img__ transformed image</span>
    <span class="comment">% * __mask__ transformed mask</span>
    <span class="comment">% * __Ai__ affine transform matrix from output/skewed `img` to input `img`</span>
    <span class="comment">%</span>

    <span class="comment">% initialize affine transformation matrix (identity)</span>
    A = eye(2,3);
    [h,w,~] = size(img);

    <span class="comment">% in the case of (tilt=1, phi=0), then no transformation (identity)</span>

    <span class="keyword">if</span> phi ~= 0
        <span class="comment">% rotate image</span>
        A = [cosd(phi) -sind(phi); sind(phi) cosd(phi)];
        corners = [0 0; w 0; w h; 0 h];
        corners = round(corners * A.');
        rect = cv.boundingRect(corners);
        A(:,3) = -rect(1:2);
        img = cv.warpAffine(img, A, <span class="string">'DSize'</span>,rect(3:4), <span class="keyword">...</span>
            <span class="string">'Interpolation'</span>,<span class="string">'Linear'</span>, <span class="string">'BorderType'</span>,<span class="string">'Replicate'</span>);
    <span class="keyword">end</span>

    <span class="keyword">if</span> tilt ~= 1
        <span class="comment">% anti-aliasing filtering in the vertical direction, then tilt image</span>
        <span class="comment">% (subsample in vertical direction by a factor of tilt)</span>
        s = 0.8 * sqrt(tilt^2 - 1);
        img = cv.GaussianBlur(img, <span class="string">'KSize'</span>,[0 0], <span class="string">'SigmaX'</span>,s, <span class="string">'SigmaY'</span>,0.01);
        img = cv.resize(img, 1/tilt, 1, <span class="string">'Interpolation'</span>,<span class="string">'Nearest'</span>);
        A(1) = A(1) / tilt;
    <span class="keyword">end</span>

    <span class="keyword">if</span> phi ~= 0 || tilt ~= 1
        <span class="comment">% apply same transformation on mask</span>
        [h,w,~] = size(img);
        mask = cv.warpAffine(mask, A, <span class="string">'DSize'</span>,[w h], <span class="string">'Interpolation'</span>,<span class="string">'Nearest'</span>);
    <span class="keyword">end</span>

    <span class="comment">% inverse affine transformation</span>
    Ai = cv.invertAffineTransform(A);
<span class="keyword">end</span>

<span class="keyword">function</span> [matches, p1, p2] = filter_matches(kp1, kp2, matches, ratio)
    <span class="comment">%FILTER_MATCHES  Filter out 2-NN matches using Lowe's ratio test</span>

    <span class="keyword">if</span> nargin &lt; 4, ratio = 0.75; <span class="keyword">end</span>

    <span class="comment">% good matches</span>
    idx = cellfun(@(m) (numel(m) == 2) &amp;&amp; <span class="keyword">...</span>
        (m(1).distance &lt; ratio * m(2).distance), matches);
    matches = cellfun(@(m) m(1), matches(idx));

    <span class="comment">% corresponding points</span>
    p1 = cat(1, kp1([matches.queryIdx]+1).pt);
    p2 = cat(1, kp2([matches.trainIdx]+1).pt);
<span class="keyword">end</span>

<span class="keyword">function</span> pos = select_poly(ax)
    <span class="comment">%SELECT_POLY  Select polygon area using the mouse</span>
    <span class="comment">%</span>
    <span class="comment">%     pos = select_poly()</span>
    <span class="comment">%     pos = select_poly(ax)</span>
    <span class="comment">%</span>
    <span class="comment">% ## Input</span>
    <span class="comment">% * __ax__ axes handle, default gca</span>
    <span class="comment">%</span>
    <span class="comment">% ## Output</span>
    <span class="comment">% * __pos__ Nx2 matrix of points</span>
    <span class="comment">%</span>

    <span class="keyword">if</span> nargin &lt; 1, ax = gca; <span class="keyword">end</span>
    hRoi = impoly(ax);
    api = iptgetapi(hRoi);
    <span class="keyword">try</span>
        api.setColor(<span class="string">'green'</span>);
        api.setPositionConstraintFcn(makeConstrainToRectFcn(<span class="string">'impoly'</span>, <span class="keyword">...</span>
            get(ax,<span class="string">'XLim'</span>).*[1 0.5], get(ax,<span class="string">'YLim'</span>)));
        pos = wait(hRoi);
    <span class="keyword">catch</span>
        pos = zeros(0,2);
    <span class="keyword">end</span>
    delete(hRoi);
<span class="keyword">end</span></pre><pre class="codeoutput">Detecting...
Elapsed time is 2.437649 seconds.
Elapsed time is 1.939506 seconds.
img1: 25629 features, img2: 17387 features
Matching...
Elapsed time is 3.822513 seconds.
25629 matches
89 good matches
16 inliers
</pre><img src="asift_demo_01.png"><div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Affine invariant feature-based image matching
%
% This sample is similar to |feature_homography_demo.m|, but uses the affine
% transformation space sampling technique, called
% <http://www.ipol.im/pub/algo/my_affine_sift/ ASIFT>. While the original
% implementation is based on SIFT, you can try to use SURF or ORB detectors
% instead. Homography RANSAC is used to reject outliers.
%
% Sources:
%
% * <https://github.com/opencv/opencv/blob/3.2.0/samples/python/asift.py>
%

function asift_demo()
    % load input grayscale images
    files = {
        fullfile(mexopencv.root(),'test','aero1.jpg')
        fullfile(mexopencv.root(),'test','aero3.jpg')
    };
    for i=1:numel(files)
        if exist(files{i}, 'file') ~= 2
            disp('Downloading...')
            url = 'https://cdn.rawgit.com/opencv/opencv/3.2.0/samples/data/';
            [~,fname,ext] = fileparts(files{i});
            urlwrite([url fname ext], files{i});
        end
    end
    img1 = cv.imread(files{1}, 'Grayscale',true);
    img2 = cv.imread(files{2}, 'Grayscale',true);

    % ASIFT
    [vis, H] = run_asift(img1, img2, 'BRISK', true);
    imshow(vis), title('ASIFT')

    % interactive exploration
    if ~mexopencv.isOctave() && mexopencv.require('images')
        % interactively select region in img1
        pos1 = select_poly(gca) - 1;
        if isempty(pos1), return; end


        % apply homography
        pos2 = cv.perspectiveTransform(pos1, H);
        pos2(:,1) = pos2(:,1) + size(img1,2);

        % draw region in img1 and transformed region in img2
        vis = cv.polylines(vis, pos1, 'Closed',true, ...
            'Color',[0 255 0], 'Thickness',2, 'LineType','AA');
        vis = cv.polylines(vis, pos2, 'Closed',true, ...
            'Color',[0 255 0], 'Thickness',2, 'LineType','AA');
        imshow(vis), title('ASIFT')
    end
end

function [vis, H] = run_asift(img1, img2, feature_name, use_flann)
    %RUN_ASIFT  Run ASIFT algorithm, estimate homography and visualize matches

    % create detector and matcher objects
    [detector, matcher] = init_feature(feature_name, use_flann);

    % detect features using ASIFT method
    disp('Detecting...')
    tic, [kp1, desc1] = affine_detect(detector, img1); toc
    tic, [kp2, desc2] = affine_detect(detector, img2); toc
    fprintf('img1: %d features, img2: %d features\n', numel(kp1), numel(kp2));

    % match ASIFT features
    disp('Matching...')
    tic, matches = matcher.knnMatch(desc1, desc2, 2); toc
    fprintf('%d matches\n', numel(matches));

    % filter matches
    [matches, p1, p2] = filter_matches(kp1, kp2, matches);
    fprintf('%d good matches\n', numel(matches));
    assert(numel(matches) >= 4, 'not enough matches for homography estimation');

    % estimate homography with RANSAC method
    [H,inliers] = cv.findHomography(p1, p2, 'Method','Ransac');
    assert(~isempty(H), 'homography estimation failed');
    inliers = logical(inliers);
    fprintf('%d inliers\n', nnz(inliers));

    % visualize good and inlier matches
    vis = cv.drawMatches(img1, kp1, img2, kp2, matches, ...
        'NotDrawSinglePoints',true, 'MatchesMask',inliers);
end

function [detector, matcher] = init_feature(feature_name, use_flann)
    %INIT_FEATURE  Create detector and matcher objects

    % detector
    switch upper(feature_name)
        case 'SURF'
            detector = cv.SURF('HessianThreshold',400);
        case 'SIFT'
            detector = cv.SIFT();
        case 'ORB'
            detector = cv.ORB();
        case 'BRISK'
            detector = cv.BRISK();
        case 'AKAZE'
            detector = cv.AKAZE();
        case 'KAZE'
            detector = cv.KAZE();
        otherwise
            error('unrecognized feature: %s', feature_name)
    end

    % matcher
    if use_flann
        if ~isempty(strfind(detector.defaultNorm(), 'Hamming'))
            opts = {'LSH', 'TableNumber',6, 'KeySize',12, 'MultiProbeLevel',1};
        else
            opts = {'KDTree', 'Trees',5};
        end
        matcher = cv.DescriptorMatcher('FlannBasedMatcher', 'Index',opts);
    else
        matcher = cv.DescriptorMatcher('BFMatcher', ...
            'NormType',detector.defaultNorm());
    end
end

function [kpts, descs] = affine_detect(detector, img, mask)
    %AFFINE_DETECT  Affine-SIFT (ASIFT) algorithm
    %
    %     [kpts, descs] = affine_detect(detector, img)
    %     [kpts, descs] = affine_detect(detector, img, mask)
    %
    % ## Input
    % * __detector__ detector object
    % * __img__ input image
    % * __mask__ optional mask, default all image included
    %
    % ## Output
    % * __kpts__ concatenated keypoints
    % * __descs__ corresponding concatenated descriptors
    %
    % Applies a set of affine transormations to the image, detect keypoints,
    % and reproject them into initial image coordinates.
    % See: http://www.ipol.im/pub/algo/my_affine_sift/ for the details.
    %

    % default mask if not specified
    if nargin < 3
        mask = 255 * ones(size(img,1), size(img,2), 'uint8');
    end

    % parameter sampling: Tilt and Phi
    % (simulates all possible affine distortions caused by the change of
    % camera optical axis orientation from a frontal position)
    params = [1 0];  % no tilt and no rotation, thus original image
    for t = sqrt(2).^(1:5)       % geometric series 1, a, a^2, .., a^n; a=sqrt(2), n=5
        for p = 0:72/t:180-72/t  % arithmetic series 0, b/t, .., k*b/t < 180; b=72
            params(end+1,:) = [t, p];
        end
    end

    % for each pair of distortion parameters
    % (this loop is candidate for parallelization, i.e parfor)
    N = size(params,1);
    kpts = cell(N, 1);
    descs = cell(N, 1);
    for i=1:N
        % transform image
        [timg, tmask, Ai] = affine_skew(params(i,1), params(i,2), img, mask);

        % detect features using a similarity invariant matching method (SIFT)
        [kp, feat] = detector.detectAndCompute(timg, 'Mask',tmask);

        %TODO: Remove keypoints close to the boundary of the transformed image

        % project keypoints from the coordinates of the rotated and tilted
        % image back to the original image corrdinates
        for j=1:numel(kp)
            kp(j).pt = [kp(j).pt, 1] * Ai.';
        end
        kpts{i} = kp(:);
        descs{i} = feat;
    end

    % concatenate all features from all simulated images
    kpts = cat(1, kpts{:});
    descs = cat(1, descs{:});
end

function [img, mask, Ai] = affine_skew(tilt, phi, img, mask)
    %AFFINE_SKEW  Transform image/mask by an affine distortion
    %
    %     [img, mask, Ai] = affine_skew(tilt, phi, img, mask)
    %
    % ## Input
    % * __tilt__ tilt
    % * __phi__ rotation angle in degrees
    % * __img__ image
    % * __mask__ mask
    %
    % ## Output
    % * __img__ transformed image
    % * __mask__ transformed mask
    % * __Ai__ affine transform matrix from output/skewed `img` to input `img`
    %

    % initialize affine transformation matrix (identity)
    A = eye(2,3);
    [h,w,~] = size(img);

    % in the case of (tilt=1, phi=0), then no transformation (identity)

    if phi ~= 0
        % rotate image
        A = [cosd(phi) -sind(phi); sind(phi) cosd(phi)];
        corners = [0 0; w 0; w h; 0 h];
        corners = round(corners * A.');
        rect = cv.boundingRect(corners);
        A(:,3) = -rect(1:2);
        img = cv.warpAffine(img, A, 'DSize',rect(3:4), ...
            'Interpolation','Linear', 'BorderType','Replicate');
    end

    if tilt ~= 1
        % anti-aliasing filtering in the vertical direction, then tilt image
        % (subsample in vertical direction by a factor of tilt)
        s = 0.8 * sqrt(tilt^2 - 1);
        img = cv.GaussianBlur(img, 'KSize',[0 0], 'SigmaX',s, 'SigmaY',0.01);
        img = cv.resize(img, 1/tilt, 1, 'Interpolation','Nearest');
        A(1) = A(1) / tilt;
    end

    if phi ~= 0 || tilt ~= 1
        % apply same transformation on mask
        [h,w,~] = size(img);
        mask = cv.warpAffine(mask, A, 'DSize',[w h], 'Interpolation','Nearest');
    end

    % inverse affine transformation
    Ai = cv.invertAffineTransform(A);
end

function [matches, p1, p2] = filter_matches(kp1, kp2, matches, ratio)
    %FILTER_MATCHES  Filter out 2-NN matches using Lowe's ratio test

    if nargin < 4, ratio = 0.75; end

    % good matches
    idx = cellfun(@(m) (numel(m) == 2) && ...
        (m(1).distance < ratio * m(2).distance), matches);
    matches = cellfun(@(m) m(1), matches(idx));

    % corresponding points
    p1 = cat(1, kp1([matches.queryIdx]+1).pt);
    p2 = cat(1, kp2([matches.trainIdx]+1).pt);
end

function pos = select_poly(ax)
    %SELECT_POLY  Select polygon area using the mouse
    %
    %     pos = select_poly()
    %     pos = select_poly(ax)
    %
    % ## Input
    % * __ax__ axes handle, default gca
    %
    % ## Output
    % * __pos__ Nx2 matrix of points
    %

    if nargin < 1, ax = gca; end
    hRoi = impoly(ax);
    api = iptgetapi(hRoi);
    try
        api.setColor('green');
        api.setPositionConstraintFcn(makeConstrainToRectFcn('impoly', ...
            get(ax,'XLim').*[1 0.5], get(ax,'YLim')));
        pos = wait(hRoi);
    catch
        pos = zeros(0,2);
    end
    delete(hRoi);
end

##### SOURCE END #####
-->
   </body>
</html>