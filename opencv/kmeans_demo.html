<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>K-Means Clustering</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2017-11-27">
      <meta name="DC.source" content="kmeans_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">K-Means Clustering</h1>
         <!--introduction-->
         <p>An example on K-means clustering.</p>
         <p>This program demonstrates kmeans clustering. It generates an image with random points, then assigns a random number of cluster
            centers and uses kmeans to move those cluster centers to their representitive location.
         </p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/kmeans.cpp">https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/kmeans.cpp</a></li>
               <li><a href="https://docs.opencv.org/3.2.0/de/d4d/tutorial_py_kmeans_understanding.html">https://docs.opencv.org/3.2.0/de/d4d/tutorial_py_kmeans_understanding.html</a></li>
            </ul>
         </div>
         <!--/introduction-->
         <h2 id="toc">Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Theory</a></li>
               <li><a href="#3">Initialization</a></li>
               <li><a href="#4">Data</a></li>
               <li><a href="#7">Clustering</a></li>
            </ul>
         </div>
         <h2 id="2">Theory</h2>
         <p>We will explain with an example which is commonly used, the T-shirt size problem.</p>
         <p>Consider a company, which is going to release a new model of T-shirt to market. Obviously they will have to manufacture models
            in different sizes to satisfy people of all sizes. So the company make a data of people's height and weight, and plot them
            on to a graph, as below:
         </p>
         <p><img src="https://docs.opencv.org/3.2.0/tshirt.jpg"></p>
         <p>Company can't create t-shirts with all the sizes. Instead, they divide people to Small, Medium and Large, and manufacture
            only these 3 models which will fit into all the people. This grouping of people into three groups can be done by k-means clustering,
            and algorithm provides us best 3 sizes, which will satisfy all the people. And if it doesn't, company can divide people to
            more groups, may be five, and so on. Check image below :
         </p>
         <p><img src="https://docs.opencv.org/3.2.0/tshirt_grouped.jpg"></p>
         <p>This algorithm is an iterative process. We will explain it step-by-step with the help of images.</p>
         <p>Consider a set of data as below (you can consider it as t-shirt problem). We need to cluster this data into two groups.</p>
         <p><img src="https://docs.opencv.org/3.2.0/testdata.jpg"></p>
         <p><b>Step 1:</b> Algorithm randomly chooses two centroids, <img src="kmeans_demo_eq17735519399197887899.png" alt="$C1$" class="equation" width="18" height="11"> and <img src="kmeans_demo_eq06127794937922056062.png" alt="$C2$" class="equation" width="18" height="11"> (sometimes, any two data are taken as the centroids).
         </p>
         <p><b>Step 2:</b> It calculates the distance from each point to both centroids. If a test data is more closer to <img src="kmeans_demo_eq17735519399197887899.png" alt="$C1$" class="equation" width="18" height="11">, then that data is labelled with <tt>0</tt>. If it is closer to <img src="kmeans_demo_eq06127794937922056062.png" alt="$C2$" class="equation" width="18" height="11">, then labelled as <tt>1</tt> (If more centroids are there, labelled as <tt>2</tt>,|3| etc).
         </p>
         <p>In our case, we will color all <tt>0</tt> labelled with red, and <tt>1</tt> labelled with blue. So we get following image after above operations.
         </p>
         <p><img src="https://docs.opencv.org/3.2.0/initial_labelling.jpg"></p>
         <p><b>Step 3:</b> Next we calculate the average of all blue points and red points separately and that will be our new centroids. That is <img src="kmeans_demo_eq17735519399197887899.png" alt="$C1$" class="equation" width="18" height="11"> and <img src="kmeans_demo_eq06127794937922056062.png" alt="$C2$" class="equation" width="18" height="11"> shift to newly calculated centroids. (Remember, the images shown are not true values and not to true scale, it is just for
            demonstration only).
         </p>
         <p>And again, perform step 2 with new centroids and label data to <tt>0</tt> and <tt>1</tt>. So we get result as below :
         </p>
         <p><img src="https://docs.opencv.org/3.2.0/update_centroid.jpg"></p>
         <p>Now step 2 and 3 are iterated until both centroids are converged to fixed points. (Or it may be stopped depending on the criteria
            we provide, like maximum number of iterations, or a specific accuracy is reached etc.). These points are such that sum of
            distances between test data and their corresponding centroids are minimum. Or simply, sum of distances between <img src="kmeans_demo_eq01630592550962156454.png" alt="$C1 \leftrightarrow Red\_Points$" class="equation" width="117" height="11"> and <img src="kmeans_demo_eq00222693533357686633.png" alt="$C2 \leftrightarrow Blue\_Points$" class="equation" width="123" height="11"> is minimum.
         </p>
         <p><img src="kmeans_demo_eq13363236391827166866.png" alt="$$J = \sum_{All\: Red\_Points} distance(C1,Red\_Point) +&#xA;      \sum_{All\: Blue\_Points} distance(C2,Blue\_Point)$$" class="equation" width="522" height="33"></p>
         <p>Final result almost looks like below :</p>
         <p><img src="https://docs.opencv.org/3.2.0/final_clusters.jpg"></p>
         <p>So this is just an intuitive understanding of K-Means Clustering. For more details and mathematical explanation, please read
            any standard machine learning textbooks or check the links below. It is just a top layer of K-Means clustering. There are
            a lot of modifications to this algorithm like, how to choose the initial centroids, how to speed up the iteration process,
            etc.
         </p>
         <div>
            <ul>
               <li><a href="https://www.coursera.org/course/ml">Machine Learning Course</a>, Video   lectures by Prof. Andrew Ng (some of the images are taken from it)
               </li>
            </ul>
         </div>
         <h2 id="3">Initialization</h2><pre class="codeinput"><span class="comment">% parameters</span>
MAX_CLUSTERS = 5;
colorTab = [<span class="keyword">...</span>
    0,   0, 255 ;
    0, 255,   0 ;
  255, 100, 100 ;
  255,   0, 255 ;
    0, 255, 255
];

<span class="comment">% image on which to draw points</span>
sz = [200 300];  <span class="comment">% height/width, rows/cols</span>
img = zeros([sz, 3], <span class="string">'uint8'</span>);

<span class="comment">% seed RNG for reproducible results</span>
<span class="keyword">try</span>, rng(<span class="string">'default'</span>); <span class="keyword">end</span></pre><h2 id="4">Data</h2>
         <p>choose number of samples and clusters</p><pre class="codeinput">sampleCount = randi([1 1000]);
clusterCount = min(randi([2 MAX_CLUSTERS]), sampleCount);</pre><p>generate random samples from multigaussian distribution</p><pre class="codeinput">points = zeros([sampleCount,2], <span class="string">'single'</span>);
idx = fix(linspace(1, sampleCount+1, clusterCount+1));
<span class="keyword">for</span> i=1:numel(idx)-1
    center = [randi([0, sz(2)]) randi([0, sz(1)])];
    sigma = diag([sz(2) sz(1)]).*0.4;
    points(idx(i):idx(i+1)-1,:) = mvnrnd(center, sigma, idx(i+1)-idx(i));
<span class="keyword">end</span></pre><p>show true labels</p><pre class="codeinput"><span class="keyword">if</span> mexopencv.isOctave()
    <span class="comment">%HACK: http://savannah.gnu.org/bugs/?45497</span>
    L = repelems(1:(numel(idx)-1), [1:(numel(idx)-1); diff(idx)]);
    <span class="comment">%HACK: GSCATTER not implemented in Octave</span>
    scatter(points(:,1), points(:,2), [], L)
<span class="keyword">else</span>
    L = repelem(1:(numel(idx)-1), diff(idx));
    gscatter(points(:,1), points(:,2), L)
<span class="keyword">end</span>
axis <span class="string">square</span> <span class="string">equal</span> <span class="string">ij</span>
axis([1 sz(2) 1 sz(1)])</pre><img src="kmeans_demo_01.png"><h2 id="7">Clustering</h2>
         <p>shuffle points</p><pre class="codeinput">points = points(randperm(sampleCount),:);</pre><p>cluster points</p><pre class="codeinput">[labels,centers,compactness] = cv.kmeans(points, clusterCount, <span class="keyword">...</span>
    <span class="string">'Criteria'</span>,struct(<span class="string">'type'</span>,<span class="string">'Count+EPS'</span>, <span class="string">'maxCount'</span>,10, <span class="string">'epsilon'</span>,0.1), <span class="keyword">...</span>
    <span class="string">'Attempts'</span>,3, <span class="string">'Initialization'</span>,<span class="string">'PP'</span>);
display(centers)</pre><pre class="codeoutput">centers =
  5&times;2 single matrix
  265.1030  143.7113
   50.0097   21.1157
   38.3347  182.2368
  123.8976   40.6695
  182.8743   78.8241
</pre><p>compactness measure of clusters</p><pre class="codeinput"><span class="comment">%compactness = cv.norm(points, centers(labels+1,:), 'NormType','L2Sqr');</span>
fprintf(<span class="string">'Compactness = %f\n'</span>, compactness);</pre><pre class="codeoutput">Compactness = 161486.122233
</pre><p>show clusters</p><pre class="codeinput">img(:) = 0;
<span class="keyword">for</span> i=1:sampleCount
    clusterIdx = labels(i)+1;
    img = cv.circle(img, points(i,:), 2, <span class="keyword">...</span>
        <span class="string">'Color'</span>,colorTab(clusterIdx,:), <span class="keyword">...</span>
        <span class="string">'Thickness'</span>,<span class="string">'Filled'</span>, <span class="string">'LineType'</span>,<span class="string">'AA'</span>);
<span class="keyword">end</span>
<span class="keyword">for</span> i=1:clusterCount
    img = cv.circle(img, centers(i,:), 40, <span class="string">'Color'</span>,colorTab(i,:), <span class="keyword">...</span>
        <span class="string">'Thickness'</span>,1, <span class="string">'LineType'</span>,<span class="string">'AA'</span>);
<span class="keyword">end</span>
figure, imshow(img), title(<span class="string">'clusters'</span>)</pre><img src="kmeans_demo_02.png"><div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div><script type="text/x-mathjax-config">
  // https://stackoverflow.com/a/14631703/97160
  MathJax.Extension.myImg2jax = {
    version: "1.0",
    PreProcess: function (element) {
      var images = element.getElementsByTagName("img");
      for (var i = images.length - 1; i >= 0; i--) {
        var img = images[i];
        if (img.className === "equation") {
          var match = img.alt.match(/^(\$\$?)([\s\S]*)\1$/m);
          if (!match) continue;
          var script = document.createElement("script");
          script.type = "math/tex";
          if (match[1] === "$$") {script.type += ";mode=display"}
          MathJax.HTML.setScript(script, match[2]);
          img.parentNode.replaceChild(script, img);
        }
      }
    }
  };
  MathJax.Hub.Register.PreProcessor(["PreProcess", MathJax.Extension.myImg2jax]);
  </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
      <!--
##### SOURCE BEGIN #####
%% K-Means Clustering
% An example on K-means clustering.
%
% This program demonstrates kmeans clustering.
% It generates an image with random points, then assigns a random number of
% cluster centers and uses kmeans to move those cluster centers to their
% representitive location.
%
% Sources:
%
% * <https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/kmeans.cpp>
% * <https://docs.opencv.org/3.2.0/de/d4d/tutorial_py_kmeans_understanding.html>
%

%% Theory
%
% We will explain with an example which is commonly used, the T-shirt size
% problem.
%
% Consider a company, which is going to release a new model of T-shirt to
% market. Obviously they will have to manufacture models in different sizes to
% satisfy people of all sizes. So the company make a data of people's height
% and weight, and plot them on to a graph, as below:
%
% <<https://docs.opencv.org/3.2.0/tshirt.jpg>>
%
% Company can't create t-shirts with all the sizes. Instead, they divide
% people to Small, Medium and Large, and manufacture only these 3 models which
% will fit into all the people. This grouping of people into three groups can
% be done by k-means clustering, and algorithm provides us best 3 sizes, which
% will satisfy all the people. And if it doesn't, company can divide people to
% more groups, may be five, and so on. Check image below :
%
% <<https://docs.opencv.org/3.2.0/tshirt_grouped.jpg>>
%
% This algorithm is an iterative process. We will explain it step-by-step with
% the help of images.
%
% Consider a set of data as below (you can consider it as t-shirt problem).
% We need to cluster this data into two groups.
%
% <<https://docs.opencv.org/3.2.0/testdata.jpg>>
%
% *Step 1:* Algorithm randomly chooses two centroids, $C1$ and $C2$
% (sometimes, any two data are taken as the centroids).
%
% *Step 2:* It calculates the distance from each point to both centroids. If a
% test data is more closer to $C1$, then that data is labelled with |0|. If it
% is closer to $C2$, then labelled as |1| (If more centroids are there,
% labelled as |2|,|3| etc).
%
% In our case, we will color all |0| labelled with red, and |1| labelled with
% blue. So we get following image after above operations.
%
% <<https://docs.opencv.org/3.2.0/initial_labelling.jpg>>
%
% *Step 3:* Next we calculate the average of all blue points and red points
% separately and that will be our new centroids. That is $C1$ and $C2$ shift
% to newly calculated centroids. (Remember, the images shown are not true
% values and not to true scale, it is just for demonstration only).
%
% And again, perform step 2 with new centroids and label data to |0| and |1|.
% So we get result as below :
%
% <<https://docs.opencv.org/3.2.0/update_centroid.jpg>>
%
% Now step 2 and 3 are iterated until both centroids are converged to fixed
% points. (Or it may be stopped depending on the criteria we provide, like
% maximum number of iterations, or a specific accuracy is reached etc.).
% These points are such that sum of distances between test data and their
% corresponding centroids are minimum. Or simply, sum of distances between
% $C1 \leftrightarrow Red\_Points$ and $C2 \leftrightarrow Blue\_Points$ is
% minimum.
%
% $$J = \sum_{All\: Red\_Points} distance(C1,Red\_Point) +
%       \sum_{All\: Blue\_Points} distance(C2,Blue\_Point)$$
%
% Final result almost looks like below :
%
% <<https://docs.opencv.org/3.2.0/final_clusters.jpg>>
%
% So this is just an intuitive understanding of K-Means Clustering. For more
% details and mathematical explanation, please read any standard machine
% learning textbooks or check the links below. It is just a top layer of
% K-Means clustering. There are a lot of modifications to this algorithm like,
% how to choose the initial centroids, how to speed up the iteration process,
% etc.
%
% * <https://www.coursera.org/course/ml Machine Learning Course>, Video
%   lectures by Prof. Andrew Ng (some of the images are taken from it)
%

%% Initialization

% parameters
MAX_CLUSTERS = 5;
colorTab = [...
    0,   0, 255 ;
    0, 255,   0 ;
  255, 100, 100 ;
  255,   0, 255 ;
    0, 255, 255
];

% image on which to draw points
sz = [200 300];  % height/width, rows/cols
img = zeros([sz, 3], 'uint8');

% seed RNG for reproducible results
try, rng('default'); end

%% Data
% choose number of samples and clusters
sampleCount = randi([1 1000]);
clusterCount = min(randi([2 MAX_CLUSTERS]), sampleCount);

%%
% generate random samples from multigaussian distribution
points = zeros([sampleCount,2], 'single');
idx = fix(linspace(1, sampleCount+1, clusterCount+1));
for i=1:numel(idx)-1
    center = [randi([0, sz(2)]) randi([0, sz(1)])];
    sigma = diag([sz(2) sz(1)]).*0.4;
    points(idx(i):idx(i+1)-1,:) = mvnrnd(center, sigma, idx(i+1)-idx(i));
end

%%
% show true labels
if mexopencv.isOctave()
    %HACK: http://savannah.gnu.org/bugs/?45497
    L = repelems(1:(numel(idx)-1), [1:(numel(idx)-1); diff(idx)]);
    %HACK: GSCATTER not implemented in Octave
    scatter(points(:,1), points(:,2), [], L)
else
    L = repelem(1:(numel(idx)-1), diff(idx));
    gscatter(points(:,1), points(:,2), L)
end
axis square equal ij
axis([1 sz(2) 1 sz(1)])

%% Clustering
% shuffle points
points = points(randperm(sampleCount),:);

%%
% cluster points
[labels,centers,compactness] = cv.kmeans(points, clusterCount, ...
    'Criteria',struct('type','Count+EPS', 'maxCount',10, 'epsilon',0.1), ...
    'Attempts',3, 'Initialization','PP');
display(centers)

%%
% compactness measure of clusters
%compactness = cv.norm(points, centers(labels+1,:), 'NormType','L2Sqr');
fprintf('Compactness = %f\n', compactness);

%%
% show clusters
img(:) = 0;
for i=1:sampleCount
    clusterIdx = labels(i)+1;
    img = cv.circle(img, points(i,:), 2, ...
        'Color',colorTab(clusterIdx,:), ...
        'Thickness','Filled', 'LineType','AA');
end
for i=1:clusterCount
    img = cv.circle(img, centers(i,:), 40, 'Color',colorTab(i,:), ...
        'Thickness',1, 'LineType','AA');
end
figure, imshow(img), title('clusters')

##### SOURCE END #####
--></body>
</html>