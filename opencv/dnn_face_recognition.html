<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>DNN Face Detection and Recognition</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2018-02-20">
      <meta name="DC.source" content="dnn_face_recognition.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">DNN Face Detection and Recognition</h1>
         <!--introduction-->
         <p>This tutorial will show us how to run deep learning models, with face detection and face recognition models pipeline.</p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://docs.opencv.org/3.4.0/d5/d86/tutorial_dnn_javascript.html">https://docs.opencv.org/3.4.0/d5/d86/tutorial_dnn_javascript.html</a></li>
               <li><a href="https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/js_face_recognition.html">https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/js_face_recognition.html</a></li>
            </ul>
         </div>
         <!--/introduction-->
         <h2 id="toc">Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Face detection</a></li>
               <li><a href="#3">Face recognition</a></li>
               <li><a href="#4">Code</a></li>
            </ul>
         </div>
         <h2 id="2">Face detection</h2>
         <p>Face detection network gets BGR image as input and produces set of bounding boxes that might contain faces. All that we need
            is just select the boxes with a strong confidence.
         </p>
         <p>Face detector is based on SSD framework (Single Shot MultiBox Detector), using a reduced ResNet-10 model.</p>
         <h2 id="3">Face recognition</h2>
         <p>Network is called <a href="https://github.com/cmusatyalab/openface">OpenFace</a>. Face recognition model receives RGB face image of size <tt>96x96</tt>. Then it returns 128-dimensional unit vector that represents input face as a point on the unit multidimensional sphere. So
            difference between two faces is an angle between two output vectors.
         </p>
         <h2 id="4">Code</h2>
         <p>Start the demo, then press "Add a person" to name a person that is recognized as an unknown one.</p>
         <p>import deep learning models</p><pre class="codeinput">[netDet, blobDetOpts] = ResNetSSD_FaceDetector();
[netRec, blobRecOpts] = OpenFace_Embedding();
assert(~netDet.empty() &amp;&amp; ~netRec.empty());</pre><p>options</p><pre class="codeinput">confThreshold = 0.5;  <span class="comment">% minimum confidence for face detection</span>
scoreThreshold = 0.5; <span class="comment">% minimum score for face recognition</span></pre><p>prepare video input</p><pre class="codeinput">cap = createVideoCapture([], <span class="string">'lena'</span>);
pause(1);
assert(cap.isOpened(), <span class="string">'Could not initialize capturing'</span>);
frame = cap.read();
assert(~isempty(frame), <span class="string">'Could not read frame'</span>);</pre><p>prepare figure</p><pre class="codeinput">hThumb = [];
hImg = imshow(frame);
hFig = ancestor(hImg, <span class="string">'figure'</span>);
setappdata(hFig, <span class="string">'flag'</span>,false);
hBtn = uicontrol(<span class="string">'Parent'</span>,hFig, <span class="string">'Style'</span>,<span class="string">'pushbutton'</span>, <span class="keyword">...</span>
    <span class="string">'Position'</span>,[20 20 100 20], <span class="string">'String'</span>,<span class="string">'Add a person'</span>, <span class="keyword">...</span>
    <span class="string">'Callback'</span>,@(~,~) setappdata(hFig, <span class="string">'flag'</span>,true));</pre><img src="dnn_face_recognition_01.png"><p>dataset of extracted faces and corresponding names</p><pre class="codeinput">vecs = zeros(128,0,<span class="string">'single'</span>);        <span class="comment">% feature vectors</span>
names = cell(1,0);
faces = zeros([96 96 3 0], <span class="string">'uint8'</span>); <span class="comment">% only needed for visualization</span></pre><p>main loop that receives a frames from a camera and makes a recognition of every detected face on the frame</p><pre class="codeinput"><span class="keyword">while</span> ishghandle(hImg)
    <span class="comment">% read frame</span>
    frame = cap.read();
    <span class="keyword">if</span> isempty(frame), <span class="keyword">break</span>; <span class="keyword">end</span>
    out = frame;

    <span class="comment">% detect faces</span>
    rects = detectFaces(frame, netDet, blobDetOpts, confThreshold);
    <span class="keyword">for</span> i=1:size(rects,1)
        <span class="comment">% preprocess face</span>
        rect = rects(i,:);
        face = alignFace(cv.Rect.crop(frame, rect));

        <span class="comment">% recognize face</span>
        vec = face2vec(face, netRec, blobRecOpts);
        [name, score] = recognizeFace(vec, vecs, names, scoreThreshold);

        <span class="comment">% show detection and prediction</span>
        out = insertAnnotation(out, rect, sprintf(<span class="string">'%s (%.2f)'</span>, name, score), <span class="keyword">...</span>
            <span class="string">'Color'</span>,name2clr(name, names), <span class="string">'TextColor'</span>,[255 255 255], <span class="keyword">...</span>
            <span class="string">'Thickness'</span>,2, <span class="string">'FontScale'</span>,0.9);
    <span class="keyword">end</span>

    <span class="comment">% update plot</span>
    set(hImg, <span class="string">'CData'</span>,out);
    drawnow;

    <span class="comment">% check if add-a-person button is pressed</span>
    flag = ishghandle(hFig) &amp;&amp; getappdata(hFig, <span class="string">'flag'</span>);
    <span class="keyword">if</span> flag &amp;&amp; ~isempty(rects)
        setappdata(hFig, <span class="string">'flag'</span>,false);

        <span class="comment">% prompt for name</span>
        name = inputdlg(<span class="string">'Enter person name:'</span>);
        name = strtrim(name{1});

        <span class="comment">% face representation as feature vector</span>
        rect = rects(1,:);
        face = alignFace(cv.Rect.crop(frame, rect));
        vec = face2vec(face, netRec, blobRecOpts);

        <span class="comment">% store</span>
        vecs(:,end+1) = vec;
        names{end+1} = name;

        <span class="comment">% visualize face + name</span>
        face = cv.resize(face, [96 96]);
        face = cv.putText(face, name, [5 25], <span class="keyword">...</span>
            <span class="string">'Color'</span>,name2clr(name, names), <span class="string">'FontScale'</span>,0.6, <span class="string">'Thickness'</span>,2);
        faces(:,:,:,end+1) = face;

        <span class="comment">% show montage of tracked people</span>
        <span class="keyword">if</span> ishghandle(hThumb)
            clf(hThumb, <span class="string">'reset'</span>);
            figure(hThumb)
        <span class="keyword">else</span>
            hThumb = figure;
        <span class="keyword">end</span>
        montage(faces, <span class="string">'Size'</span>,[NaN 2]);
        movegui(hThumb, <span class="string">'east'</span>)
    <span class="keyword">end</span>
<span class="keyword">end</span>
cap.release();</pre><p>Helper functions</p><pre class="codeinput"><span class="keyword">function</span> [rects, confs] = detectFaces(img, net, blobOpts, thresh)
    <span class="comment">%DETECTFACES  Run face detection network to detect faces on input image</span>
    <span class="comment">%</span>
    <span class="comment">% You may play with input blob sizes to balance detection quality and</span>
    <span class="comment">% efficiency. The bigger input blob the smaller faces may be detected.</span>
    <span class="comment">%</span>

    <span class="comment">% detect faces</span>
    net.setInput(cv.Net.blobFromImages(flip(img,3), blobOpts{:}));
    dets = net.forward();

    <span class="comment">% SSD output is 1-by-1-by-ndetections-by-7</span>
    <span class="comment">% d = [img_id, class_id, confidence, left, bottom, right, top]</span>
    dets = permute(dets, [3 4 2 1]);

    <span class="comment">% filter out weak detections</span>
    <span class="keyword">if</span> nargin &lt; 4, thresh = 0.5; <span class="keyword">end</span>
    idx = (dets(:,2) == 1 &amp; dets(:,3) &gt; thresh);  <span class="comment">% 0: background, 1: face</span>
    dets = dets(idx,:);

    <span class="comment">% adjust relative coordinates to image size</span>
    sz = [size(img,2) size(img,1)];
    dets(:,4:7) = bsxfun(@times, dets(:,4:7), [sz sz]);

    <span class="comment">% output detections (clamp coords and remove small and out-of-bound rects)</span>
    rects = cv.Rect.from2points(dets(:,4:5), dets(:,6:7));
    rects = cv.Rect.intersect(rects, [0 0 sz]);
    idx = (cv.Rect.area(rects) &gt;= 10);
    rects = rects(idx,:);
    confs = dets(idx,3);
<span class="keyword">end</span>

<span class="keyword">function</span> img = alignFace(img)
    <span class="comment">%ALIGNFACE  Align face to make the eyes and bottom lip appear in the same location</span>
    <span class="comment">%</span>
    <span class="comment">% OpenFace expects faces to be aligned, it uses Dlib.</span>
    <span class="comment">%</span>

    <span class="comment">%TODO: not implemented, maybe we could port this:</span>
    <span class="comment">% https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/</span>

    <span class="comment">%TODO: we could also use facial landmarks from opencv_contrib face module</span>
    <span class="comment">% (cv.Facemark and cv.FacemarkKazemi)</span>
<span class="keyword">end</span>

<span class="keyword">function</span> vec = face2vec(img, net, blobOpts)
    <span class="comment">%FACE2VEC  Get 128 floating points feature vector</span>
    <span class="comment">%</span>
    <span class="comment">% Run face recognition network to receive 128-dimensional unit feature</span>
    <span class="comment">% vector from input face image.</span>
    <span class="comment">%</span>

    net.setInput(cv.Net.blobFromImages(img, blobOpts{:}));
    vec = net.forward();
    vec = vec(:);
<span class="keyword">end</span>

<span class="keyword">function</span> [name, score] = recognizeFace(vec, vecs, names, thresh)
    <span class="comment">%RECOGNIZEFACE  Perform face recognition</span>
    <span class="comment">%</span>
    <span class="comment">% Match a new feature vector with registered ones. Return a name of the</span>
    <span class="comment">% best matched person.</span>
    <span class="comment">%</span>
    <span class="comment">% (NOTE: For more advanced usage, we could train an SVM classifier)</span>
    <span class="comment">%</span>
    <span class="comment">% See also: pdist2</span>
    <span class="comment">%</span>

    <span class="keyword">if</span> nargin &lt; 4, thresh = 0.5; <span class="keyword">end</span>
    name = <span class="string">'unknown'</span>;
    score = -1;

    <span class="keyword">if</span> ~isempty(vecs)
        scores = vec.' * vecs;  <span class="comment">% dot-product of vec against each vecs(:,i)</span>
        [s, idx] = max(scores);
        <span class="keyword">if</span> s &gt; thresh
            name = names{idx};
            score = s;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> clr = name2clr(name, names)
    clrs = round(255 * lines(7));
    idx = find(strcmp(name, names));
    <span class="keyword">if</span> isempty(idx)
        clr = [128 128 128];
    <span class="keyword">else</span>
        idx = rem(idx - 1, 7) + 1;
        clr = clrs(idx,:);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> img = insertAnnotation(img, rect, str, varargin)
    <span class="comment">% See also: insertObjectAnnotation, insertShape, insertText</span>
    p = inputParser();
    p.addParameter(<span class="string">'Alpha'</span>, 0.6);
    p.addParameter(<span class="string">'Thickness'</span>, 1);
    p.addParameter(<span class="string">'Color'</span>, [255 255 0]);
    p.addParameter(<span class="string">'TextColor'</span>, [0 0 0]);
    p.addParameter(<span class="string">'FontFace'</span>, <span class="string">'HersheySimplex'</span>);
    p.addParameter(<span class="string">'FontScale'</span>, 0.4);
    p.addParameter(<span class="string">'AntiAlias'</span>, true);
    p.addParameter(<span class="string">'Shape'</span>, <span class="string">'rectangle'</span>);
    p.parse(varargin{:});
    opts = p.Results;
    opts.Shape = validatestring(opts.Shape, {<span class="string">'rectangle'</span>,<span class="string">'circle'</span>});
    thick = 1;

    [sz,b] = cv.getTextSize(str, <span class="string">'Thickness'</span>,thick, <span class="keyword">...</span>
        <span class="string">'FontFace'</span>,opts.FontFace, <span class="string">'FontScale'</span>,opts.FontScale);
    txt_rect = [rect(1), rect(2)-sz(2)-b, sz(1), sz(2)+b];
    txt_orig = [rect(1), rect(2)-b];

    <span class="keyword">if</span> opts.AntiAlias
        alias = {<span class="string">'LineType'</span>,<span class="string">'AA'</span>};
    <span class="keyword">else</span>
        alias = {<span class="string">'LineType'</span>,8};
    <span class="keyword">end</span>

    overlay = img;
    <span class="keyword">if</span> strcmp(opts.Shape, <span class="string">'rectangle'</span>)
        overlay = cv.rectangle(overlay, rect, <span class="keyword">...</span>
            <span class="string">'Color'</span>,opts.Color, <span class="string">'Thickness'</span>,opts.Thickness, alias{:});
    <span class="keyword">else</span>
        c = rect(1:2) + rect(3:4)/2;
        r = max(rect(3:4)/2);
        overlay = cv.circle(overlay, c, r, <span class="keyword">...</span>
            <span class="string">'Color'</span>,opts.Color, <span class="string">'Thickness'</span>,opts.Thickness, alias{:});
    <span class="keyword">end</span>
    overlay = cv.rectangle(overlay, txt_rect, <span class="keyword">...</span>
        <span class="string">'Color'</span>,opts.Color, <span class="string">'Thickness'</span>,<span class="string">'Filled'</span>, alias{:});
    <span class="keyword">if</span> opts.Thickness &gt; 1
        overlay = cv.rectangle(overlay, txt_rect, <span class="keyword">...</span>
            <span class="string">'Color'</span>,opts.Color, <span class="string">'Thickness'</span>,opts.Thickness, alias{:});
    <span class="keyword">end</span>
    overlay = cv.putText(overlay, str, txt_orig, <span class="keyword">...</span>
        <span class="string">'FontFace'</span>,opts.FontFace, <span class="string">'FontScale'</span>,opts.FontScale, <span class="keyword">...</span>
        <span class="string">'Color'</span>,opts.TextColor, <span class="string">'Thickness'</span>,thick, alias{:});

    img = cv.addWeighted(img,1-opts.Alpha, overlay,opts.Alpha, 0);
<span class="keyword">end</span></pre><img src="dnn_face_recognition_02.png"><img src="dnn_face_recognition_03.png"><p>Pretrained models</p><pre class="codeinput"><span class="keyword">function</span> dname = get_dnn_dir(dname)
    <span class="comment">%GET_DNN_DIR  Path to model files, and show where to get them if missing</span>

    dname = fullfile(mexopencv.root(), <span class="string">'test'</span>, <span class="string">'dnn'</span>, dname);
    b = isdir(dname);
    <span class="keyword">if</span> ~b
        <span class="comment">% display help of calling function</span>
        <span class="comment">% (assumed to be a local function in current file)</span>
        st = dbstack(1);
        help([mfilename() filemarker() st(1).name])
    <span class="keyword">end</span>
    assert(b, <span class="string">'Missing model: %s'</span>, dname);
<span class="keyword">end</span>

<span class="keyword">function</span> [net, blobOpts] = ResNetSSD_FaceDetector()
    <span class="comment">%RESNETSSD_FACEDETECTOR  face detector based on SSD framework with reduced ResNet-10 backbone</span>
    <span class="comment">%</span>
    <span class="comment">% homepage = https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/face_detector/how_to_train_face_detector.txt</span>
    <span class="comment">%</span>
    <span class="comment">% ## Model</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/ResNetSSD_FaceDetector/deploy.prototxt</span>
    <span class="comment">% url  = https://github.com/opencv/opencv/raw/3.4.0/samples/dnn/face_detector/deploy.prototxt</span>
    <span class="comment">% hash = 006BAF926232DF6F6332DEFB9C24F94BB9F3764E</span>
    <span class="comment">%</span>
    <span class="comment">% ## Weights</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/ResNetSSD_FaceDetector/res10_300x300_ssd_iter_140000.caffemodel</span>
    <span class="comment">% url  = https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel</span>
    <span class="comment">% hash = 15aa726b4d46d9f023526d85537db81cbc8dd566</span>
    <span class="comment">% size = 10.1 MB</span>
    <span class="comment">%</span>

    dname = get_dnn_dir(<span class="string">'ResNetSSD_FaceDetector'</span>);
    net = cv.Net(<span class="string">'Caffe'</span>, <span class="keyword">...</span>
        fullfile(dname, <span class="string">'deploy.prototxt'</span>), <span class="keyword">...</span>
        fullfile(dname, <span class="string">'res10_300x300_ssd_iter_140000.caffemodel'</span>));
    blobOpts = {<span class="string">'SwapRB'</span>,false, <span class="string">'Crop'</span>,false, <span class="string">'Size'</span>,[300 300], <span class="string">'Mean'</span>,[104 117 123]};
<span class="keyword">end</span>

<span class="keyword">function</span> [net, blobOpts] = OpenFace_Embedding()
    <span class="comment">%OPENFACE  OpenFace embedding for face recognition</span>
    <span class="comment">%</span>
    <span class="comment">% homepage = https://cmusatyalab.github.io/openface/</span>
    <span class="comment">%</span>
    <span class="comment">% ## Model + Weights</span>
    <span class="comment">%</span>
    <span class="comment">% file = test/dnn/OpenFace/nn4.small2.v1.t7</span>
    <span class="comment">% url  = https://storage.cmusatyalab.org/openface-models/nn4.small2.v1.t7</span>
    <span class="comment">% hash = ac8161a4376fb5a79ceec55d85bbb57ef81da9fe</span>
    <span class="comment">% size = 30 MB</span>
    <span class="comment">%</span>

    dname = get_dnn_dir(<span class="string">'OpenFace'</span>);
    net = cv.Net(<span class="string">'Torch'</span>, fullfile(dname, <span class="string">'nn4.small2.v1.t7'</span>));
    blobOpts = {<span class="string">'SwapRB'</span>,false, <span class="string">'Crop'</span>,false, <span class="string">'Size'</span>,[96 96], <span class="string">'ScaleFactor'</span>,1/255};
<span class="keyword">end</span></pre><div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div>
      <!--
##### SOURCE BEGIN #####
%% DNN Face Detection and Recognition
% This tutorial will show us how to run deep learning models, with face
% detection and face recognition models pipeline.
%
% Sources:
%
% * <https://docs.opencv.org/3.4.0/d5/d86/tutorial_dnn_javascript.html>
% * <https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/js_face_recognition.html>
%
%% Face detection
% Face detection network gets BGR image as input and produces set of bounding
% boxes that might contain faces. All that we need is just select the boxes
% with a strong confidence.
%
% Face detector is based on SSD framework (Single Shot MultiBox Detector),
% using a reduced ResNet-10 model.
%
%% Face recognition
% Network is called <https://github.com/cmusatyalab/openface OpenFace>. Face
% recognition model receives RGB face image of size |96x96|. Then it returns
% 128-dimensional unit vector that represents input face as a point on the
% unit multidimensional sphere. So difference between two faces is an angle
% between two output vectors.
%
%% Code
% Start the demo, then press "Add a person" to name a person that is
% recognized as an unknown one.
%

%%
% import deep learning models
[netDet, blobDetOpts] = ResNetSSD_FaceDetector();
[netRec, blobRecOpts] = OpenFace_Embedding();
assert(~netDet.empty() && ~netRec.empty());

%%
% options
confThreshold = 0.5;  % minimum confidence for face detection
scoreThreshold = 0.5; % minimum score for face recognition

%%
% prepare video input
cap = createVideoCapture([], 'lena');
pause(1);
assert(cap.isOpened(), 'Could not initialize capturing');
frame = cap.read();
assert(~isempty(frame), 'Could not read frame');

%%
% prepare figure
hThumb = [];
hImg = imshow(frame);
hFig = ancestor(hImg, 'figure');
setappdata(hFig, 'flag',false);
hBtn = uicontrol('Parent',hFig, 'Style','pushbutton', ...
    'Position',[20 20 100 20], 'String','Add a person', ...
    'Callback',@(~,~) setappdata(hFig, 'flag',true));

%%
% dataset of extracted faces and corresponding names
vecs = zeros(128,0,'single');        % feature vectors
names = cell(1,0);
faces = zeros([96 96 3 0], 'uint8'); % only needed for visualization

%%
% main loop that receives a frames from a camera and makes a recognition of
% every detected face on the frame
while ishghandle(hImg)
    % read frame
    frame = cap.read();
    if isempty(frame), break; end
    out = frame;

    % detect faces
    rects = detectFaces(frame, netDet, blobDetOpts, confThreshold);
    for i=1:size(rects,1)
        % preprocess face
        rect = rects(i,:);
        face = alignFace(cv.Rect.crop(frame, rect));

        % recognize face
        vec = face2vec(face, netRec, blobRecOpts);
        [name, score] = recognizeFace(vec, vecs, names, scoreThreshold);

        % show detection and prediction
        out = insertAnnotation(out, rect, sprintf('%s (%.2f)', name, score), ...
            'Color',name2clr(name, names), 'TextColor',[255 255 255], ...
            'Thickness',2, 'FontScale',0.9);
    end

    % update plot
    set(hImg, 'CData',out);
    drawnow;

    % check if add-a-person button is pressed
    flag = ishghandle(hFig) && getappdata(hFig, 'flag');
    if flag && ~isempty(rects)
        setappdata(hFig, 'flag',false);

        % prompt for name
        name = inputdlg('Enter person name:');
        name = strtrim(name{1});

        % face representation as feature vector
        rect = rects(1,:);
        face = alignFace(cv.Rect.crop(frame, rect));
        vec = face2vec(face, netRec, blobRecOpts);

        % store
        vecs(:,end+1) = vec;
        names{end+1} = name;

        % visualize face + name
        face = cv.resize(face, [96 96]);
        face = cv.putText(face, name, [5 25], ...
            'Color',name2clr(name, names), 'FontScale',0.6, 'Thickness',2);
        faces(:,:,:,end+1) = face;

        % show montage of tracked people
        if ishghandle(hThumb)
            clf(hThumb, 'reset');
            figure(hThumb)
        else
            hThumb = figure;
        end
        montage(faces, 'Size',[NaN 2]);
        movegui(hThumb, 'east')
    end
end
cap.release();

%%
% Helper functions

function [rects, confs] = detectFaces(img, net, blobOpts, thresh)
    %DETECTFACES  Run face detection network to detect faces on input image
    %
    % You may play with input blob sizes to balance detection quality and
    % efficiency. The bigger input blob the smaller faces may be detected.
    %

    % detect faces
    net.setInput(cv.Net.blobFromImages(flip(img,3), blobOpts{:}));
    dets = net.forward();

    % SSD output is 1-by-1-by-ndetections-by-7
    % d = [img_id, class_id, confidence, left, bottom, right, top]
    dets = permute(dets, [3 4 2 1]);

    % filter out weak detections
    if nargin < 4, thresh = 0.5; end
    idx = (dets(:,2) == 1 & dets(:,3) > thresh);  % 0: background, 1: face
    dets = dets(idx,:);

    % adjust relative coordinates to image size
    sz = [size(img,2) size(img,1)];
    dets(:,4:7) = bsxfun(@times, dets(:,4:7), [sz sz]);

    % output detections (clamp coords and remove small and out-of-bound rects)
    rects = cv.Rect.from2points(dets(:,4:5), dets(:,6:7));
    rects = cv.Rect.intersect(rects, [0 0 sz]);
    idx = (cv.Rect.area(rects) >= 10);
    rects = rects(idx,:);
    confs = dets(idx,3);
end

function img = alignFace(img)
    %ALIGNFACE  Align face to make the eyes and bottom lip appear in the same location
    %
    % OpenFace expects faces to be aligned, it uses Dlib.
    %

    %TODO: not implemented, maybe we could port this:
    % https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/

    %TODO: we could also use facial landmarks from opencv_contrib face module
    % (cv.Facemark and cv.FacemarkKazemi)
end

function vec = face2vec(img, net, blobOpts)
    %FACE2VEC  Get 128 floating points feature vector
    %
    % Run face recognition network to receive 128-dimensional unit feature
    % vector from input face image.
    %

    net.setInput(cv.Net.blobFromImages(img, blobOpts{:}));
    vec = net.forward();
    vec = vec(:);
end

function [name, score] = recognizeFace(vec, vecs, names, thresh)
    %RECOGNIZEFACE  Perform face recognition
    %
    % Match a new feature vector with registered ones. Return a name of the
    % best matched person.
    %
    % (NOTE: For more advanced usage, we could train an SVM classifier)
    %
    % See also: pdist2
    %

    if nargin < 4, thresh = 0.5; end
    name = 'unknown';
    score = -1;

    if ~isempty(vecs)
        scores = vec.' * vecs;  % dot-product of vec against each vecs(:,i)
        [s, idx] = max(scores);
        if s > thresh
            name = names{idx};
            score = s;
        end
    end
end

function clr = name2clr(name, names)
    clrs = round(255 * lines(7));
    idx = find(strcmp(name, names));
    if isempty(idx)
        clr = [128 128 128];
    else
        idx = rem(idx - 1, 7) + 1;
        clr = clrs(idx,:);
    end
end

function img = insertAnnotation(img, rect, str, varargin)
    % See also: insertObjectAnnotation, insertShape, insertText
    p = inputParser();
    p.addParameter('Alpha', 0.6);
    p.addParameter('Thickness', 1);
    p.addParameter('Color', [255 255 0]);
    p.addParameter('TextColor', [0 0 0]);
    p.addParameter('FontFace', 'HersheySimplex');
    p.addParameter('FontScale', 0.4);
    p.addParameter('AntiAlias', true);
    p.addParameter('Shape', 'rectangle');
    p.parse(varargin{:});
    opts = p.Results;
    opts.Shape = validatestring(opts.Shape, {'rectangle','circle'});
    thick = 1;

    [sz,b] = cv.getTextSize(str, 'Thickness',thick, ...
        'FontFace',opts.FontFace, 'FontScale',opts.FontScale);
    txt_rect = [rect(1), rect(2)-sz(2)-b, sz(1), sz(2)+b];
    txt_orig = [rect(1), rect(2)-b];

    if opts.AntiAlias
        alias = {'LineType','AA'};
    else
        alias = {'LineType',8};
    end

    overlay = img;
    if strcmp(opts.Shape, 'rectangle')
        overlay = cv.rectangle(overlay, rect, ...
            'Color',opts.Color, 'Thickness',opts.Thickness, alias{:});
    else
        c = rect(1:2) + rect(3:4)/2;
        r = max(rect(3:4)/2);
        overlay = cv.circle(overlay, c, r, ...
            'Color',opts.Color, 'Thickness',opts.Thickness, alias{:});
    end
    overlay = cv.rectangle(overlay, txt_rect, ...
        'Color',opts.Color, 'Thickness','Filled', alias{:});
    if opts.Thickness > 1
        overlay = cv.rectangle(overlay, txt_rect, ...
            'Color',opts.Color, 'Thickness',opts.Thickness, alias{:});
    end
    overlay = cv.putText(overlay, str, txt_orig, ...
        'FontFace',opts.FontFace, 'FontScale',opts.FontScale, ...
        'Color',opts.TextColor, 'Thickness',thick, alias{:});

    img = cv.addWeighted(img,1-opts.Alpha, overlay,opts.Alpha, 0);
end

%%
% Pretrained models

function dname = get_dnn_dir(dname)
    %GET_DNN_DIR  Path to model files, and show where to get them if missing

    dname = fullfile(mexopencv.root(), 'test', 'dnn', dname);
    b = isdir(dname);
    if ~b
        % display help of calling function
        % (assumed to be a local function in current file)
        st = dbstack(1);
        help([mfilename() filemarker() st(1).name])
    end
    assert(b, 'Missing model: %s', dname);
end

function [net, blobOpts] = ResNetSSD_FaceDetector()
    %RESNETSSD_FACEDETECTOR  face detector based on SSD framework with reduced ResNet-10 backbone
    %
    % homepage = https://github.com/opencv/opencv/blob/3.4.0/samples/dnn/face_detector/how_to_train_face_detector.txt
    %
    % ## Model
    %
    % file = test/dnn/ResNetSSD_FaceDetector/deploy.prototxt
    % url  = https://github.com/opencv/opencv/raw/3.4.0/samples/dnn/face_detector/deploy.prototxt
    % hash = 006BAF926232DF6F6332DEFB9C24F94BB9F3764E
    %
    % ## Weights
    %
    % file = test/dnn/ResNetSSD_FaceDetector/res10_300x300_ssd_iter_140000.caffemodel
    % url  = https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel
    % hash = 15aa726b4d46d9f023526d85537db81cbc8dd566
    % size = 10.1 MB
    %

    dname = get_dnn_dir('ResNetSSD_FaceDetector');
    net = cv.Net('Caffe', ...
        fullfile(dname, 'deploy.prototxt'), ...
        fullfile(dname, 'res10_300x300_ssd_iter_140000.caffemodel'));
    blobOpts = {'SwapRB',false, 'Crop',false, 'Size',[300 300], 'Mean',[104 117 123]};
end

function [net, blobOpts] = OpenFace_Embedding()
    %OPENFACE  OpenFace embedding for face recognition
    %
    % homepage = https://cmusatyalab.github.io/openface/
    %
    % ## Model + Weights
    %
    % file = test/dnn/OpenFace/nn4.small2.v1.t7
    % url  = https://storage.cmusatyalab.org/openface-models/nn4.small2.v1.t7
    % hash = ac8161a4376fb5a79ceec55d85bbb57ef81da9fe
    % size = 30 MB
    %

    dname = get_dnn_dir('OpenFace');
    net = cv.Net('Torch', fullfile(dname, 'nn4.small2.v1.t7'));
    blobOpts = {'SwapRB',false, 'Crop',false, 'Size',[96 96], 'ScaleFactor',1/255};
end

##### SOURCE END #####
-->
   </body>
</html>