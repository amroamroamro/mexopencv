<!DOCTYPE html>
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--This HTML was auto-generated from published MATLAB code.-->
      <title>Principal Component Analysis (PCA)</title>
      <meta name="generator" content="MATLAB 9.2">
      <link rel="schema.DC" href="http://purl.org/dc/elements/1.1/">
      <meta name="DC.date" content="2017-11-27">
      <meta name="DC.source" content="pca_intro_demo.m">
      <link rel="stylesheet" type="text/css" href="publish_custom.css">
   </head>
   <body>
      <div class="content">
         <h1 id="1">Principal Component Analysis (PCA)</h1>
         <!--introduction-->
         <p>In this demo, you will learn how to use the OpenCV class <tt>cv.PCA</tt> to calculate the orientation of an object.
         </p>
         <p>Sources:</p>
         <div>
            <ul>
               <li><a href="https://docs.opencv.org/3.2.0/d1/dee/tutorial_introduction_to_pca.html">https://docs.opencv.org/3.2.0/d1/dee/tutorial_introduction_to_pca.html</a></li>
               <li><a href="https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp">https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp</a></li>
            </ul>
         </div>
         <!--/introduction-->
         <h2 id="toc">Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Theory</a></li>
               <li><a href="#3">Code</a></li>
               <li><a href="#9">Helper functions</a></li>
               <li><a href="#10">References</a></li>
            </ul>
         </div>
         <h2 id="2">Theory</h2>
         <p>Principal Component Analysis (PCA) is a statistical procedure that extracts the most important features of a dataset.</p>
         <p><img src="https://docs.opencv.org/3.2.0/pca_line.png"></p>
         <p>Consider that you have a set of 2D points as it is shown in the figure above. Each dimension corresponds to a feature you
            are interested in. Here some could argue that the points are set in a random order. However, if you have a better look you
            will see that there is a linear pattern (indicated by the blue line) which is hard to dismiss. A key point of PCA is the Dimensionality
            Reduction. Dimensionality Reduction is the process of reducing the number of the dimensions of the given dataset. For example,
            in the above case it is possible to approximate the set of points to a single line and therefore, reduce the dimensionality
            of the given points from 2D to 1D.
         </p>
         <p>Moreover, you could also see that the points vary the most along the blue line, more than they vary along the Feature 1 or
            Feature 2 axes. This means that if you know the position of a point along the blue line you have more information about the
            point than if you only knew where it was on Feature 1 axis or Feature 2 axis.
         </p>
         <p>Hence, PCA allows us to find the direction along which our data varies the most. In fact, the result of running PCA on the
            set of points in the diagram consist of 2 vectors called <i>eigenvectors</i> which are the <i>principal components</i> of the data set.
         </p>
         <p><img src="https://docs.opencv.org/3.2.0/pca_eigen.png"></p>
         <p>The size of each eigenvector is encoded in the corresponding eigenvalue and indicates how much the data vary along the principal
            component. The beginning of the eigenvectors is the center of all points in the data set. Applying PCA to N-dimensional data
            set yields N N-dimensional eigenvectors, N eigenvalues and 1 N-dimensional center point.
         </p>
         <p>Now we will see how the eigenvectors and eigenvalues are computed.</p>
         <p>The goal is to transform a given data set <img src="pca_intro_demo_eq12362013959998143435.png" alt="$X$" class="equation" width="13" height="10"> of dimension <img src="pca_intro_demo_eq09941021013676836398.png" alt="$p$" class="equation" width="8" height="10"> to an alternative data set <img src="pca_intro_demo_eq07649107795707013777.png" alt="$Y$" class="equation" width="11" height="10"> of smaller dimension <img src="pca_intro_demo_eq11904963258706611165.png" alt="$L$" class="equation" width="9" height="10">. Equivalently, we are seeking to find the matrix <img src="pca_intro_demo_eq07649107795707013777.png" alt="$Y$" class="equation" width="11" height="10">, where <img src="pca_intro_demo_eq07649107795707013777.png" alt="$Y$" class="equation" width="11" height="10"> is the <i>Karhunen-Loeve transform</i> (KLT) of matrix <img src="pca_intro_demo_eq12362013959998143435.png" alt="$X$" class="equation" width="13" height="10">:
         </p>
         <p><img src="pca_intro_demo_eq00090398386084918589.png" alt="$$ \mathbf{Y} = \bf{K} \bf{L} \bf{T} \{\mathbf{X}\} $$" class="equation" width="93" height="15"></p>
         <p><b>1) Organize the data set</b></p>
         <p>Suppose you have data comprising a set of observations of <img src="pca_intro_demo_eq09941021013676836398.png" alt="$p$" class="equation" width="8" height="10"> variables, and you want to reduce the data so that each observation can be described with only <img src="pca_intro_demo_eq11904963258706611165.png" alt="$L$" class="equation" width="9" height="10"> variables, <img src="pca_intro_demo_eq06612673028775322647.png" alt="$L < p$" class="equation" width="37" height="13">. Suppose further, that the data are arranged as a set of <img src="pca_intro_demo_eq08984225997457563733.png" alt="$n$" class="equation" width="9" height="7"> data vectors <img src="pca_intro_demo_eq16430115698155765434.png" alt="$x_1...x_n$" class="equation" width="41" height="9"> with each <img src="pca_intro_demo_eq08383031602117067423.png" alt="$x_i$" class="equation" width="12" height="9">  representing a single grouped observation of the <img src="pca_intro_demo_eq09941021013676836398.png" alt="$p$" class="equation" width="8" height="10"> variables.
         </p>
         <div>
            <ul>
               <li>Write <img src="pca_intro_demo_eq16430115698155765434.png" alt="$x_1...x_n$" class="equation" width="41" height="9"> as row vectors, each of which has <img src="pca_intro_demo_eq09941021013676836398.png" alt="$p$" class="equation" width="8" height="10"> columns.
               </li>
               <li>Place the row vectors into a single matrix <img src="pca_intro_demo_eq12362013959998143435.png" alt="$X$" class="equation" width="13" height="10"> of dimensions <img src="pca_intro_demo_eq04531020536506235259.png" alt="$n \times p$" class="equation" width="34" height="11">.
               </li>
            </ul>
         </div>
         <p><b>2) Calculate the empirical mean</b></p>
         <div>
            <ul>
               <li>Find the empirical mean along each dimension <img src="pca_intro_demo_eq06363188988862147740.png" alt="$j = 1, ..., p$" class="equation" width="67" height="13">.
               </li>
               <li>Place the calculated mean values into an empirical mean vector <img src="pca_intro_demo_eq11776305044305525613.png" alt="$u$" class="equation" width="8" height="7"> of   dimensions <img src="pca_intro_demo_eq03223062992576788894.png" alt="$p \times 1$" class="equation" width="32" height="13">.
               </li>
            </ul>
         </div>
         <p><img src="pca_intro_demo_eq00223517483549786858.png" alt="$$ \mathbf{u[j]} = \frac{1}{n}\sum_{i=1}^{n}\mathbf{X[i,j]} $$" class="equation" width="116" height="41"></p>
         <p><b>3) Calculate the deviations from the mean</b></p>
         <p>Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square
            error of approximating the data. Hence, we proceed by centering the data as follows:
         </p>
         <div>
            <ul>
               <li>Subtract the empirical mean vector <img src="pca_intro_demo_eq11776305044305525613.png" alt="$u$" class="equation" width="8" height="7"> from each row of the data matrix   <img src="pca_intro_demo_eq12362013959998143435.png" alt="$X$" class="equation" width="13" height="10">.
               </li>
               <li>Store mean-subtracted data in the <img src="pca_intro_demo_eq04531020536506235259.png" alt="$n \times p$" class="equation" width="34" height="11"> matrix <img src="pca_intro_demo_eq10170753361147586657.png" alt="$B$" class="equation" width="11" height="10">.
               </li>
            </ul>
         </div>
         <p><img src="pca_intro_demo_eq07708437410332628785.png" alt="$$ \mathbf{B} = \mathbf{X} - \mathbf{h}\mathbf{u^{T}} $$" class="equation" width="89" height="14"></p>
         <p>where <img src="pca_intro_demo_eq08062630503172331818.png" alt="$h$" class="equation" width="8" height="11"> is an <img src="pca_intro_demo_eq17754908661825192061.png" alt="$n \times 1$" class="equation" width="33" height="11"> column vector of all 1s:
         </p>
         <p><img src="pca_intro_demo_eq04422473592509322109.png" alt="$$ h[i] = 1, i = 1, ..., n $$" class="equation" width="120" height="15"></p>
         <p><b>4) Find the covariance matrix</b></p>
         <div>
            <ul>
               <li>Find the <img src="pca_intro_demo_eq14277226881934273385.png" alt="$p \times p$" class="equation" width="33" height="11"> empirical covariance matrix <img src="pca_intro_demo_eq03986222445007418011.png" alt="$C$" class="equation" width="11" height="11"> from the outer   product of matrix <img src="pca_intro_demo_eq10170753361147586657.png" alt="$B$" class="equation" width="11" height="10"> with itself:
               </li>
            </ul>
         </div>
         <p><img src="pca_intro_demo_eq10152206768608969126.png" alt="$$ \mathbf{C} = \frac{1}{n-1} \mathbf{B^{*}} \cdot \mathbf{B} $$" class="equation" width="109" height="31"></p>
         <p>where <img src="pca_intro_demo_eq09007014681540195452.png" alt="$*$" class="equation" width="6" height="7"> is the conjugate transpose operator. Note that if <img src="pca_intro_demo_eq10170753361147586657.png" alt="$B$" class="equation" width="11" height="10"> consists entirely of real numbers, which is the case in many applications, the "conjugate transpose" is the same as the regular
            transpose.
         </p>
         <p><b>5) Find the eigenvectors and eigenvalues of the covariance matrix</b></p>
         <div>
            <ul>
               <li>Compute the matrix <img src="pca_intro_demo_eq02739270504201626537.png" alt="$V$" class="equation" width="11" height="11"> of eigenvectors which diagonalizes the covariance   matrix <img src="pca_intro_demo_eq03986222445007418011.png" alt="$C$" class="equation" width="11" height="11">:
               </li>
            </ul>
         </div>
         <p><img src="pca_intro_demo_eq07680960402042857648.png" alt="$$ \mathbf{V^{-1}} \mathbf{C} \mathbf{V} = \mathbf{D} $$" class="equation" width="84" height="14"></p>
         <p>where <img src="pca_intro_demo_eq16171345478199012472.png" alt="$D$" class="equation" width="12" height="10"> is the diagonal matrix of eigenvalues of <img src="pca_intro_demo_eq03986222445007418011.png" alt="$C$" class="equation" width="11" height="11">.
         </p>
         <div>
            <ul>
               <li>Matrix <img src="pca_intro_demo_eq16171345478199012472.png" alt="$D$" class="equation" width="12" height="10"> will take the form of an <img src="pca_intro_demo_eq14277226881934273385.png" alt="$p \times p$" class="equation" width="33" height="11"> diagonal matrix:
               </li>
            </ul>
         </div>
         <p><img src="pca_intro_demo_eq10630659301502877323.png" alt="$$ D[k,l] = \left\{\matrix{ \lambda_k, k = l \cr 0, k \neq l }\right. $$" class="equation" width="126" height="36"></p>
         <p>here, <img src="pca_intro_demo_eq18126417077771028807.png" alt="$\lambda_j$" class="equation" width="13" height="15"> is the <img src="pca_intro_demo_eq07225361342133155126.png" alt="$j$" class="equation" width="6" height="13">-th eigenvalue of the covariance matrix <img src="pca_intro_demo_eq03986222445007418011.png" alt="$C$" class="equation" width="11" height="11"></p>
         <div>
            <ul>
               <li>Matrix <img src="pca_intro_demo_eq02739270504201626537.png" alt="$V$" class="equation" width="11" height="11">, also of dimension <img src="pca_intro_demo_eq14277226881934273385.png" alt="$p \times p$" class="equation" width="33" height="11">, contains <img src="pca_intro_demo_eq09941021013676836398.png" alt="$p$" class="equation" width="8" height="10"> column vectors,   each of length <img src="pca_intro_demo_eq09941021013676836398.png" alt="$p$" class="equation" width="8" height="10">, which represent the <img src="pca_intro_demo_eq09941021013676836398.png" alt="$p$" class="equation" width="8" height="10"> eigenvectors of the covariance   matrix <img src="pca_intro_demo_eq03986222445007418011.png" alt="$C$" class="equation" width="11" height="11">.
               </li>
               <li>The eigenvalues and eigenvectors are ordered and paired. The <img src="pca_intro_demo_eq07225361342133155126.png" alt="$j$" class="equation" width="6" height="13">-th   eigenvalue corresponds to the <img src="pca_intro_demo_eq07225361342133155126.png" alt="$j$" class="equation" width="6" height="13"> th eigenvector.
               </li>
            </ul>
         </div>
         <h2 id="3">Code</h2><pre class="codeinput"><span class="keyword">function</span> pca_intro_demo()</pre><p>Load image</p><pre class="codeinput">    fname = fullfile(mexopencv.root(), <span class="string">'test'</span>, <span class="string">'pca_test1.jpg'</span>);
    <span class="keyword">if</span> exist(fname, <span class="string">'file'</span>) ~= 2
        disp(<span class="string">'Downloading image...'</span>)
        url = <span class="string">'https://cdn.rawgit.com/opencv/opencv/3.2.0/samples/data/pca_test1.jpg'</span>;
        urlwrite(url, fname);
    <span class="keyword">end</span>
    src = cv.imread(fname, <span class="string">'Color'</span>,true);</pre><p>Convert image to binary</p><pre class="codeinput">    bw = cv.threshold(cv.cvtColor(src,<span class="string">'RGB2GRAY'</span>), <span class="string">'Otsu'</span>);</pre><p>Find all the contours in the thresholded image</p><pre class="codeinput">    [contours, hierarchy] = cv.findContours(bw, <span class="string">'Mode'</span>,<span class="string">'List'</span>, <span class="string">'Method'</span>,<span class="string">'None'</span>);
    <span class="keyword">for</span> i=1:numel(contours)
        <span class="comment">% Calculate the area of each contour</span>
        a = cv.contourArea(contours{i});
        <span class="keyword">if</span> a &lt; 1e2 || a &gt; 1e5
            <span class="comment">% Ignore contours that are too small or too large</span>
            <span class="keyword">continue</span>;
        <span class="keyword">end</span>

        <span class="comment">% Draw each contour only for visualisation purposes</span>
        src = cv.drawContours(src, contours, <span class="string">'Hierarchy'</span>,hierarchy, <span class="keyword">...</span>
            <span class="string">'ContourIdx'</span>,i-1, <span class="string">'MaxLevel'</span>,0, <span class="string">'Color'</span>,[255 0 0], <span class="string">'Thickness'</span>,2);

        <span class="comment">% Find the orientation of each shape</span>
        [src, ang] = getOrientation(src, contours{i});
        fprintf(<span class="string">'Contour #%d orientation = %.1f degrees\n'</span>, i, -ang*(180/pi));
    <span class="keyword">end</span></pre><p>Show result</p><pre class="codeinput">    imshow(src)</pre><img src="pca_intro_demo_01.png"><pre class="codeinput"><span class="keyword">end</span></pre><h2 id="9">Helper functions</h2><pre class="codeinput"><span class="keyword">function</span> [img, angl] = getOrientation(img, pts)
    <span class="comment">%GETORIENTATION  Extract object orientation using PCA</span>

    <span class="comment">% Perform PCA analysis data points arranged as Nx2</span>
    pts = cat(1, pts{:});
    obj = cv.PCA(pts);

    <span class="comment">% Draw the object center of mass</span>
    img = cv.circle(img, obj.mean, 3, <span class="string">'Color'</span>,[255 0 255], <span class="string">'Thickness'</span>,2);

    <span class="comment">% Draw the principal components (each eigenvector is multiplied by its</span>
    <span class="comment">% eigenvalue and translated to the mean position)</span>
    s = 0.02 * bsxfun(@times, obj.eigenvectors, obj.eigenvalues);
    img = drawAxisArrow(img, obj.mean, obj.mean + s(1,:), 1, <span class="keyword">...</span>
        <span class="string">'Color'</span>,[0 255 0], <span class="string">'LineType'</span>,<span class="string">'AA'</span>);
    img = drawAxisArrow(img, obj.mean, obj.mean - s(2,:), 5, <span class="keyword">...</span>
        <span class="string">'Color'</span>,[0 255 255], <span class="string">'LineType'</span>,<span class="string">'AA'</span>);

    <span class="comment">% Orientation in radians [-pi,pi]</span>
    angl = atan2(obj.eigenvectors(1,2), obj.eigenvectors(1,1));
<span class="keyword">end</span>

<span class="keyword">function</span> img = drawAxisArrow(img, p, q, sc, varargin)
    <span class="comment">%DRAWAXISARROW  Draw principal component axis line</span>

    <span class="keyword">if</span> nargin &lt; 4, sc = 0.2; <span class="keyword">end</span>

    <span class="comment">% Compute angle (in radians) and hypotenuse</span>
    v = p - q;
    ang = atan2(v(2), v(1));
    mag = hypot(v(2), v(1));

    <span class="comment">% Here we lengthen the arrow by a factor of scale,</span>
    <span class="comment">% and draw the main line of the arrow</span>
    q = p - sc * mag * [cos(ang), sin(ang)];
    img = cv.line(img, p, q, varargin{:});

    <span class="comment">% Now we draw the tips of the arrow. We do some scaling so that the</span>
    <span class="comment">% tips look proportional to the main line of the arrow</span>
    p = q + 9 * [cos(ang + pi/4), sin(ang + pi/4)];
    img = cv.line(img, p, q, varargin{:});
    p = q + 9 * [cos(ang - pi/4), sin(ang - pi/4)];
    img = cv.line(img, p, q, varargin{:});
<span class="keyword">end</span></pre><pre class="codeoutput">Contour #1 orientation = 14.9 degrees
Contour #2 orientation = 13.5 degrees
Contour #3 orientation = 12.7 degrees
Contour #4 orientation = 9.7 degrees
Contour #5 orientation = -86.5 degrees
Contour #6 orientation = 9.8 degrees
</pre><h2 id="10">References</h2>
         <div>
            <ul>
               <li><a href="https://robospace.wordpress.com/2013/10/09/object-orientation-principal-component-analysis-opencv/">https://robospace.wordpress.com/2013/10/09/object-orientation-principal-component-analysis-opencv/</a></li>
               <li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis">https://en.wikipedia.org/wiki/Principal_component_analysis</a></li>
            </ul>
         </div>
         <p>And special thanks to Svetlin Penkov for the original tutorial.</p>
         <div class="footer">
            <p><a href="https://www.mathworks.com/products/matlab.html">Published with MATLAB&reg; R2017a</a></p>
         </div>
      </div><script type="text/x-mathjax-config">
  // https://stackoverflow.com/a/14631703/97160
  MathJax.Extension.myImg2jax = {
    version: "1.0",
    PreProcess: function (element) {
      var images = element.getElementsByTagName("img");
      for (var i = images.length - 1; i >= 0; i--) {
        var img = images[i];
        if (img.className === "equation") {
          var match = img.alt.match(/^(\$\$?)([\s\S]*)\1$/m);
          if (!match) continue;
          var script = document.createElement("script");
          script.type = "math/tex";
          if (match[1] === "$$") {script.type += ";mode=display"}
          MathJax.HTML.setScript(script, match[2]);
          img.parentNode.replaceChild(script, img);
        }
      }
    }
  };
  MathJax.Hub.Register.PreProcessor(["PreProcess", MathJax.Extension.myImg2jax]);
  </script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
      <!--
##### SOURCE BEGIN #####
%% Principal Component Analysis (PCA)
%
% In this demo, you will learn how to use the OpenCV class |cv.PCA| to
% calculate the orientation of an object.
%
% Sources:
%
% * <https://docs.opencv.org/3.2.0/d1/dee/tutorial_introduction_to_pca.html>
% * <https://github.com/opencv/opencv/blob/3.2.0/samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp>
%

%% Theory
%
% Principal Component Analysis (PCA) is a statistical procedure that extracts
% the most important features of a dataset.
%
% <<https://docs.opencv.org/3.2.0/pca_line.png>>
%
% Consider that you have a set of 2D points as it is shown in the figure
% above. Each dimension corresponds to a feature you are interested in. Here
% some could argue that the points are set in a random order. However, if you
% have a better look you will see that there is a linear pattern (indicated by
% the blue line) which is hard to dismiss. A key point of PCA is the
% Dimensionality Reduction. Dimensionality Reduction is the process of
% reducing the number of the dimensions of the given dataset. For example, in
% the above case it is possible to approximate the set of points to a single
% line and therefore, reduce the dimensionality of the given points from 2D to
% 1D.
%
% Moreover, you could also see that the points vary the most along the blue
% line, more than they vary along the Feature 1 or Feature 2 axes. This means
% that if you know the position of a point along the blue line you have more
% information about the point than if you only knew where it was on Feature 1
% axis or Feature 2 axis.
%
% Hence, PCA allows us to find the direction along which our data varies the
% most. In fact, the result of running PCA on the set of points in the diagram
% consist of 2 vectors called _eigenvectors_ which are the
% _principal components_ of the data set.
%
% <<https://docs.opencv.org/3.2.0/pca_eigen.png>>
%
% The size of each eigenvector is encoded in the corresponding eigenvalue and
% indicates how much the data vary along the principal component. The
% beginning of the eigenvectors is the center of all points in the data set.
% Applying PCA to N-dimensional data set yields N N-dimensional eigenvectors,
% N eigenvalues and 1 N-dimensional center point.
%
% Now we will see how the eigenvectors and eigenvalues are computed.
%
% The goal is to transform a given data set $X$ of dimension $p$ to an
% alternative data set $Y$ of smaller dimension $L$. Equivalently, we are
% seeking to find the matrix $Y$, where $Y$ is the _Karhunen-Loeve transform_
% (KLT) of matrix $X$:
%
% $$ \mathbf{Y} = \bf{K} \bf{L} \bf{T} \{\mathbf{X}\} $$
%
% *1) Organize the data set*
%
% Suppose you have data comprising a set of observations of $p$ variables, and
% you want to reduce the data so that each observation can be described with
% only $L$ variables, $L < p$. Suppose further, that the data are arranged as
% a set of $n$ data vectors $x_1...x_n$ with each $x_i$  representing a single
% grouped observation of the $p$ variables.
%
% * Write $x_1...x_n$ as row vectors, each of which has $p$ columns.
% * Place the row vectors into a single matrix $X$ of dimensions $n \times p$.
%
% *2) Calculate the empirical mean*
%
% * Find the empirical mean along each dimension $j = 1, ..., p$.
% * Place the calculated mean values into an empirical mean vector $u$ of
%   dimensions $p \times 1$.
%
% $$ \mathbf{u[j]} = \frac{1}{n}\sum_{i=1}^{n}\mathbf{X[i,j]} $$
%
% *3) Calculate the deviations from the mean*
%
% Mean subtraction is an integral part of the solution towards finding a
% principal component basis that minimizes the mean square error of
% approximating the data. Hence, we proceed by centering the data as follows:
%
% * Subtract the empirical mean vector $u$ from each row of the data matrix
%   $X$.
% * Store mean-subtracted data in the $n \times p$ matrix $B$.
%
% $$ \mathbf{B} = \mathbf{X} - \mathbf{h}\mathbf{u^{T}} $$
%
% where $h$ is an $n \times 1$ column vector of all 1s:
%
% $$ h[i] = 1, i = 1, ..., n $$
%
% *4) Find the covariance matrix*
%
% * Find the $p \times p$ empirical covariance matrix $C$ from the outer
%   product of matrix $B$ with itself:
%
% $$ \mathbf{C} = \frac{1}{n-1} \mathbf{B^{*}} \cdot \mathbf{B} $$
%
% where $*$ is the conjugate transpose operator. Note that if $B$ consists
% entirely of real numbers, which is the case in many applications, the
% "conjugate transpose" is the same as the regular transpose.
%
% *5) Find the eigenvectors and eigenvalues of the covariance matrix*
%
% * Compute the matrix $V$ of eigenvectors which diagonalizes the covariance
%   matrix $C$:
%
% $$ \mathbf{V^{-1}} \mathbf{C} \mathbf{V} = \mathbf{D} $$
%
% where $D$ is the diagonal matrix of eigenvalues of $C$.
%
% * Matrix $D$ will take the form of an $p \times p$ diagonal matrix:
%
% $$ D[k,l] = \left\{\matrix{ \lambda_k, k = l \cr 0, k \neq l }\right. $$
%
% here, $\lambda_j$ is the $j$-th eigenvalue of the covariance matrix $C$
%
% * Matrix $V$, also of dimension $p \times p$, contains $p$ column vectors,
%   each of length $p$, which represent the $p$ eigenvectors of the covariance
%   matrix $C$.
% * The eigenvalues and eigenvectors are ordered and paired. The $j$-th
%   eigenvalue corresponds to the $j$ th eigenvector.
%

%% Code

function pca_intro_demo()
    %%
    % Load image
    fname = fullfile(mexopencv.root(), 'test', 'pca_test1.jpg');
    if exist(fname, 'file') ~= 2
        disp('Downloading image...')
        url = 'https://cdn.rawgit.com/opencv/opencv/3.2.0/samples/data/pca_test1.jpg';
        urlwrite(url, fname);
    end
    src = cv.imread(fname, 'Color',true);

    %%
    % Convert image to binary
    bw = cv.threshold(cv.cvtColor(src,'RGB2GRAY'), 'Otsu');

    %%
    % Find all the contours in the thresholded image
    [contours, hierarchy] = cv.findContours(bw, 'Mode','List', 'Method','None');
    for i=1:numel(contours)
        % Calculate the area of each contour
        a = cv.contourArea(contours{i});
        if a < 1e2 || a > 1e5
            % Ignore contours that are too small or too large
            continue;
        end

        % Draw each contour only for visualisation purposes
        src = cv.drawContours(src, contours, 'Hierarchy',hierarchy, ...
            'ContourIdx',i-1, 'MaxLevel',0, 'Color',[255 0 0], 'Thickness',2);

        % Find the orientation of each shape
        [src, ang] = getOrientation(src, contours{i});
        fprintf('Contour #%d orientation = %.1f degrees\n', i, -ang*(180/pi));
    end

    %%
    % Show result
    imshow(src)
end

%% Helper functions

function [img, angl] = getOrientation(img, pts)
    %GETORIENTATION  Extract object orientation using PCA

    % Perform PCA analysis data points arranged as Nx2
    pts = cat(1, pts{:});
    obj = cv.PCA(pts);

    % Draw the object center of mass
    img = cv.circle(img, obj.mean, 3, 'Color',[255 0 255], 'Thickness',2);

    % Draw the principal components (each eigenvector is multiplied by its
    % eigenvalue and translated to the mean position)
    s = 0.02 * bsxfun(@times, obj.eigenvectors, obj.eigenvalues);
    img = drawAxisArrow(img, obj.mean, obj.mean + s(1,:), 1, ...
        'Color',[0 255 0], 'LineType','AA');
    img = drawAxisArrow(img, obj.mean, obj.mean - s(2,:), 5, ...
        'Color',[0 255 255], 'LineType','AA');

    % Orientation in radians [-pi,pi]
    angl = atan2(obj.eigenvectors(1,2), obj.eigenvectors(1,1));
end

function img = drawAxisArrow(img, p, q, sc, varargin)
    %DRAWAXISARROW  Draw principal component axis line

    if nargin < 4, sc = 0.2; end

    % Compute angle (in radians) and hypotenuse
    v = p - q;
    ang = atan2(v(2), v(1));
    mag = hypot(v(2), v(1));

    % Here we lengthen the arrow by a factor of scale,
    % and draw the main line of the arrow
    q = p - sc * mag * [cos(ang), sin(ang)];
    img = cv.line(img, p, q, varargin{:});

    % Now we draw the tips of the arrow. We do some scaling so that the
    % tips look proportional to the main line of the arrow
    p = q + 9 * [cos(ang + pi/4), sin(ang + pi/4)];
    img = cv.line(img, p, q, varargin{:});
    p = q + 9 * [cos(ang - pi/4), sin(ang - pi/4)];
    img = cv.line(img, p, q, varargin{:});
end

%% References
%
% * <https://robospace.wordpress.com/2013/10/09/object-orientation-principal-component-analysis-opencv/>
% * <https://en.wikipedia.org/wiki/Principal_component_analysis>
%
% And special thanks to Svetlin Penkov for the original tutorial.
%

##### SOURCE END #####
--></body>
</html>